{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3454c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import socket\n",
    "import threading\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from argparse import ArgumentParser\n",
    "os.environ['PYGAME_HIDE_SUPPORT_PROMPT'] = \"hide\"\n",
    "from pygame.time import Clock\n",
    "import pickle\n",
    "\n",
    "from articulate.math import *\n",
    "from mobileposer.models import *\n",
    "from mobileposer.utils.model_utils import *\n",
    "from mobileposer.config import *\n",
    "\n",
    "import coremltools as ct\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53cd0b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mobileposer.models.rnn import RNN\n",
    "class JointsBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs: N IMUs.\n",
    "    Outputs: 24 Joint positions. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_imu, seq_length):\n",
    "        super().__init__()\n",
    "        # self.joints = net.joints.joints\n",
    "        self.joints = RNN(n_imu, 24 * 3, 256, seq_length)\n",
    "\n",
    "    def forward(self, batch, input_lengths: Tensor):\n",
    "        # forward joint model\n",
    "        joints, _, _ = self.joints(batch)\n",
    "        return joints\n",
    "\n",
    "class PoserBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs: N IMUs.\n",
    "    Outputs: SMPL Pose Parameters (as 6D Rotations).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_output_joints, n_imu, n_reduced, seq_length):\n",
    "        super().__init__()\n",
    "        # self.pose = net.pose.pose\n",
    "        self.pose = RNN(n_output_joints*3 + n_imu, n_reduced*6, 256, seq_length)\n",
    "\n",
    "    def forward(self, batch, input_lengths: Tensor):\n",
    "        # forward the pose prediction model\n",
    "        pred_pose, _, _ = self.pose(batch)\n",
    "        return pred_pose\n",
    "\n",
    "class VelocityBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs: N IMUs.\n",
    "    Outputs: Per-Frame Root Velocity. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_output_joints, n_imu, seq_length):\n",
    "        super().__init__()\n",
    "\n",
    "        # model definitions\n",
    "        # self.vel = net.velocity.vel\n",
    "        self.vel = RNN(n_output_joints * 3 + n_imu, 24 * 3, 256, bidirectional=False, seq_length=seq_length)\n",
    "\n",
    "    def forward(self, batch, h, c, input_lengths:Tensor):\n",
    "        # forward velocity model\n",
    "        vel, _, state = self.vel(batch, (h,c))\n",
    "        h_out, c_out = state[0].detach(), state[1].detach()\n",
    "        return vel, h_out, c_out\n",
    "    \n",
    "class FootContactBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs: N IMUs.\n",
    "    Outputs: Foot Contact Probability ([s_lfoot, s_rfoot]).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_output_joints, n_imu, seq_length):\n",
    "        super().__init__()\n",
    "        # self.footcontact = net.foot_contact.footcontact\n",
    "        self.footcontact = RNN(n_output_joints * 3 + n_imu, 2, 64, seq_length=seq_length)\n",
    "\n",
    "    def forward(self, batch, input_lengths: Tensor):\n",
    "        # forward foot contact model\n",
    "        foot_contact, _, _ = self.footcontact(batch)\n",
    "        return foot_contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7c694c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobilePoserBase(nn.Module):\n",
    "    # def __init__(self, net, body_model, joints_model, pose_model, contact_model, velocity_model, n_reduced, ignored):\n",
    "    #     super().__init__()\n",
    "\n",
    "    #     #constants\n",
    "    #     self.n_reduced = n_reduced\n",
    "    #     self.ignored = ignored\n",
    "\n",
    "    #     #core model layers\n",
    "    #     self.bodymodel = body_model\n",
    "    #     self.joints = joints_model\n",
    "    #     self.pose = pose_model\n",
    "    #     self.foot_contact = contact_model\n",
    "    #     self.velocity = velocity_model\n",
    "    def __init__(self, n_reduced, ignored, n_imu, n_output_joints, seq_length):\n",
    "        super().__init__()\n",
    "\n",
    "        #constants\n",
    "        self.n_reduced = n_reduced\n",
    "        self.ignored = ignored\n",
    "\n",
    "        #core model layers\n",
    "        self.joints = JointsBase(n_imu=n_imu, seq_length=seq_length)\n",
    "        self.pose = PoserBase(n_imu=n_imu, n_output_joints=n_output_joints, n_reduced=n_reduced, seq_length=seq_length)\n",
    "        self.foot_contact = FootContactBase(n_output_joints=n_output_joints, n_imu=n_imu, seq_length=seq_length)\n",
    "        self.velocity = VelocityBase(n_imu=n_imu, n_output_joints=n_output_joints, seq_length=seq_length)\n",
    "    \n",
    "    def global_to_local_pose(self, pose: torch.Tensor) -> torch.Tensor:\n",
    "        # this runs in Python only\n",
    "        return self.bodymodel.inverse_kinematics_R(pose)\n",
    "\n",
    "    def forward(self, batch, h, c, input_lengths):\n",
    "        # forward the joint prediction model\n",
    "        pred_joints = self.joints(batch, input_lengths)\n",
    "\n",
    "        # forward the pose prediction model\n",
    "        pose_input = torch.cat((pred_joints, batch), dim=-1)\n",
    "        pred_pose = self.pose(pose_input, input_lengths)\n",
    "\n",
    "        # forward the foot-ground contact probability model\n",
    "        tran_input = torch.cat((pred_joints, batch), dim=-1)\n",
    "        foot_contact = self.foot_contact(tran_input, input_lengths)\n",
    "\n",
    "        # foward the foot-joint velocity model\n",
    "        pred_vel, velocity_h, velocity_c = self.velocity(tran_input, h, c, input_lengths)\n",
    "        pred_vel = pred_vel.squeeze(0)\n",
    "\n",
    "        pred_pose, pred_joints, pred_vel, foot_contact = self.process_base_outputs(pred_pose, pred_joints, pred_vel, foot_contact)\n",
    "\n",
    "        return pred_pose, pred_joints, pred_vel, foot_contact, velocity_h, velocity_c\n",
    "    \n",
    "    def rotation_matrix_to_axis_angle(self, r: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param r: Tensor of shape (..., 3, 3), a batch of rotation matrices\n",
    "        :return: Tensor of shape (..., 3), the corresponding axis-angle vectors\n",
    "        \"\"\"\n",
    "        # Flatten batch dims\n",
    "        R = r.view(-1, 3, 3)\n",
    "\n",
    "        # 1) compute the trace → cos θ\n",
    "        tr = R[..., 0, 0] + R[..., 1, 1] + R[..., 2, 2]\n",
    "        cos_theta = (tr - 1.0) * 0.5\n",
    "        cos_theta = cos_theta.clamp(-1.0, 1.0)\n",
    "\n",
    "        # 2) recover θ\n",
    "        theta = torch.acos(cos_theta)\n",
    "\n",
    "        # 3) compute the \"cross-differences\" v = [R32-R23, R13-R31, R21-R12]\n",
    "        rx = R[..., 2, 1] - R[..., 1, 2]\n",
    "        ry = R[..., 0, 2] - R[..., 2, 0]\n",
    "        rz = R[..., 1, 0] - R[..., 0, 1]\n",
    "        v   = torch.stack((rx, ry, rz), dim=-1)\n",
    "\n",
    "        # 4) normalize to get the rotation axis: axis = v / (2 sin θ)\n",
    "        sin_theta = torch.sin(theta).clamp(min=1e-6).unsqueeze(-1)\n",
    "        axis = v / (2.0 * sin_theta)\n",
    "\n",
    "        # 5) axis-angle vector = axis * θ\n",
    "        rot_vec = axis * theta.unsqueeze(-1)\n",
    "\n",
    "        # return rot_vec.view(R.shape[0], 3)\n",
    "        return rot_vec.flatten()\n",
    "    \n",
    "    def process_base_outputs(self, pose, pred_joints, vel, contact):\n",
    "        \n",
    "        pose = art.math.r6d_to_rotation_matrix(pose).reshape(-1, 24, 3, 3)\n",
    "        \n",
    "        # get pose\n",
    "        curr_pose = pose[40]   # shape = [24, 3, 3]\n",
    "        curr_pose = curr_pose.flatten(start_dim=2)\n",
    "        curr_pose = self.rotation_matrix_to_axis_angle(curr_pose)\n",
    "\n",
    "        # compute the joint positions from predicted pose\n",
    "        joints = pred_joints.squeeze(0)[40].view(24, 3)\n",
    "\n",
    "        # compute translation from foot-contact probability\n",
    "        contact = contact[0][40]\n",
    "        \n",
    "        # velocity from network-based estimation\n",
    "        root_vel = vel.view(-1, 24, 3)[:, 0]\n",
    "\n",
    "        pred_vel = root_vel[40] / (30/2) #hardcoded fps = 30, vel_scale = 2\n",
    "\n",
    "        # Need to implement in Swift\n",
    "\n",
    "        # lfoot_pos, rfoot_pos = joints[10], joints[11]\n",
    "        # if contact[0] > contact[1]:\n",
    "        #     contact_vel = self.last_lfoot_pos - lfoot_pos + self.gravity_velocity\n",
    "        # else:\n",
    "        #     contact_vel = self.last_rfoot_pos - rfoot_pos + self.gravity_velocity\n",
    "        # weight = self._prob_to_weight(contact.max())\n",
    "        # velocity = art.math.lerp(pred_vel, contact_vel, weight)\n",
    "        # current_foot_y = self.current_root_y + min(lfoot_pos[1].item(), rfoot_pos[1].item())\n",
    "        # if current_foot_y + velocity[1].item() <= self.floor_y:\n",
    "        #     velocity[1] = self.floor_y - current_foot_y\n",
    "\n",
    "        # self.current_root_y += velocity[1].item()\n",
    "        # self.last_lfoot_pos, self.last_rfoot_pos = lfoot_pos, rfoot_pos\n",
    "        # self.last_root_pos += velocity\n",
    "\n",
    "        return curr_pose, joints, pred_vel, contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "166630d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = MobilePoserBase(\n",
    "                    n_reduced=joint_set.n_full,\n",
    "                    # n_reduced=joint_set.n_reduced,\n",
    "                    ignored=joint_set.ignored,\n",
    "                    n_imu=model_config.n_imu,\n",
    "                    n_output_joints=model_config.n_output_joints,\n",
    "                    seq_length=torch.tensor([model_config.past_frames+model_config.future_frames]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d8653c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobilePoserBase(\n",
       "  (joints): JointsBase(\n",
       "    (joints): RNN(\n",
       "      (rnn): LSTM(256, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "      (linear1): Linear(in_features=60, out_features=256, bias=True)\n",
       "      (linear2): Linear(in_features=512, out_features=72, bias=True)\n",
       "      (dropout): Dropout(p=0.4, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (pose): PoserBase(\n",
       "    (pose): RNN(\n",
       "      (rnn): LSTM(256, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "      (linear1): Linear(in_features=132, out_features=256, bias=True)\n",
       "      (linear2): Linear(in_features=512, out_features=144, bias=True)\n",
       "      (dropout): Dropout(p=0.4, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (foot_contact): FootContactBase(\n",
       "    (footcontact): RNN(\n",
       "      (rnn): LSTM(64, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       "      (linear1): Linear(in_features=132, out_features=64, bias=True)\n",
       "      (linear2): Linear(in_features=128, out_features=2, bias=True)\n",
       "      (dropout): Dropout(p=0.4, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (velocity): VelocityBase(\n",
       "    (vel): RNN(\n",
       "      (rnn): LSTM(256, 256, num_layers=2, batch_first=True)\n",
       "      (linear1): Linear(in_features=132, out_features=256, bias=True)\n",
       "      (linear2): Linear(in_features=256, out_features=72, bias=True)\n",
       "      (dropout): Dropout(p=0.4, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.load_state_dict(torch.load(\"/Users/brianchen/Research/MobilePoser/mobileposer/checkpoints/model_finetuned.pth\", map_location=\"cpu\"))\n",
    "# base.load_state_dict(torch.load(\"/Users/brianchen/Research/MobilePoser/mobileposer/checkpoints/weights.pth\"))\n",
    "base.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "feb48c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=MobilePoserBase\n",
       "  (joints): RecursiveScriptModule(\n",
       "    original_name=JointsBase\n",
       "    (joints): RecursiveScriptModule(\n",
       "      original_name=RNN\n",
       "      (rnn): RecursiveScriptModule(original_name=LSTM)\n",
       "      (linear1): RecursiveScriptModule(original_name=Linear)\n",
       "      (linear2): RecursiveScriptModule(original_name=Linear)\n",
       "      (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "    )\n",
       "  )\n",
       "  (pose): RecursiveScriptModule(\n",
       "    original_name=PoserBase\n",
       "    (pose): RecursiveScriptModule(\n",
       "      original_name=RNN\n",
       "      (rnn): RecursiveScriptModule(original_name=LSTM)\n",
       "      (linear1): RecursiveScriptModule(original_name=Linear)\n",
       "      (linear2): RecursiveScriptModule(original_name=Linear)\n",
       "      (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "    )\n",
       "  )\n",
       "  (foot_contact): RecursiveScriptModule(\n",
       "    original_name=FootContactBase\n",
       "    (footcontact): RecursiveScriptModule(\n",
       "      original_name=RNN\n",
       "      (rnn): RecursiveScriptModule(original_name=LSTM)\n",
       "      (linear1): RecursiveScriptModule(original_name=Linear)\n",
       "      (linear2): RecursiveScriptModule(original_name=Linear)\n",
       "      (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "    )\n",
       "  )\n",
       "  (velocity): RecursiveScriptModule(\n",
       "    original_name=VelocityBase\n",
       "    (vel): RecursiveScriptModule(\n",
       "      original_name=RNN\n",
       "      (rnn): RecursiveScriptModule(original_name=LSTM)\n",
       "      (linear1): RecursiveScriptModule(original_name=Linear)\n",
       "      (linear2): RecursiveScriptModule(original_name=Linear)\n",
       "      (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripted_core = torch.jit.script(base)\n",
    "scripted_core.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a440acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_past_frames = model_config.past_frames\n",
    "num_future_frames = model_config.future_frames\n",
    "num_total_frames = num_past_frames + num_future_frames\n",
    "data = torch.zeros((60))\n",
    "imu = data.repeat(num_total_frames, 1)\n",
    "\n",
    "input_length = torch.tensor([num_total_frames])\n",
    "imu_input = imu.unsqueeze(0)\n",
    "h, c = (torch.zeros((2, 1, 256)), torch.zeros((2, 1, 256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d5d36ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "base.eval()\n",
    "with torch.no_grad():\n",
    "    out = base(imu_input, h, c, input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57b5507d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "When both 'convert_to' and 'minimum_deployment_target' not specified, 'convert_to' is set to \"mlprogram\" and 'minimum_deployment_target' is set to ct.target.iOS15 (which is same as ct.target.macOS12). Note: the model will not run on systems older than iOS15/macOS12/watchOS8/tvOS15. In order to make your model run on older system, please set the 'minimum_deployment_target' to iOS14/iOS13. Details please see the link: https://apple.github.io/coremltools/docs-guides/source/target-conversion-formats.html\n",
      "Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting PyTorch Frontend ==> MIL Ops:  99%|█████████▉| 307/309 [00:00<00:00, 1007.03 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 97.49 passes/s]\n",
      "Running MIL default pipeline:  11%|█         | 10/89 [00:00<00:00, 98.56 passes/s]/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '187', of the source model, has been renamed to 'var_187' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '188', of the source model, has been renamed to 'var_188' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '385', of the source model, has been renamed to 'var_385' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '394', of the source model, has been renamed to 'var_394' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '400', of the source model, has been renamed to 'var_400' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '418', of the source model, has been renamed to 'var_418' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL default pipeline: 100%|██████████| 89/89 [00:01<00:00, 79.08 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 226.17 passes/s]\n"
     ]
    }
   ],
   "source": [
    "scripted_core = scripted_core.eval()\n",
    "traced_core = torch.jit.trace(base, (imu_input, h, c, input_length))\n",
    "model_from_trace = ct.convert(\n",
    "    traced_core,\n",
    "    inputs=[ct.TensorType(shape=imu_input.shape), ct.TensorType(shape=h.shape), ct.TensorType(shape=c.shape), ct.TensorType(shape=input_length.shape)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1692e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model as a Core ML model package\n",
    "output_descriptions = [(\"pred_pose\", \"global pose output, need to convert global reduced to local full pose\"),\n",
    "                                        (\"pred_joints\", \"joint predictions\"),\n",
    "                                         (\"pred_vel\", \"velocity predictions\"),\n",
    "                                          (\"foot_contact\", \"foot contact prediction\"),\n",
    "                                           (\"velocity_h\", \"hidden states for velocity lstm\"),\n",
    "                                           (\"velocity_c\", \"initial cell states for velocity lstm\")]\n",
    "model_from_trace.save(\"MobilePoser.mlpackage\")\n",
    "# Load the saved model\n",
    "loaded_model = ct.models.MLModel(\"MobilePoser.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8d3672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = model_from_trace.get_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da9f05a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for out, (name, desc) in zip(model_from_trace._spec.description.output, output_descriptions):\n",
    "    out.shortDescription = desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7bbc549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.transforms import quaternion_to_matrix, matrix_to_quaternion\n",
    "\n",
    "class Quaternion2Matrix(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def quaternion_to_matrix(self, quaternions: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        r, i, j, k = torch.unbind(quaternions, -1)\n",
    "        # pyre-fixme[58]: `/` is not supported for operand types `float` and `Tensor`.\n",
    "        two_s = 2.0 / (quaternions * quaternions).sum(-1)\n",
    "\n",
    "        o = torch.stack(\n",
    "            (\n",
    "                1 - two_s * (j * j + k * k),\n",
    "                two_s * (i * j - k * r),\n",
    "                two_s * (i * k + j * r),\n",
    "                two_s * (i * j + k * r),\n",
    "                1 - two_s * (i * i + k * k),\n",
    "                two_s * (j * k - i * r),\n",
    "                two_s * (i * k - j * r),\n",
    "                two_s * (j * k + i * r),\n",
    "                1 - two_s * (i * i + j * j),\n",
    "            ),\n",
    "            -1,\n",
    "        )\n",
    "        return o.reshape(quaternions.shape[:-1] + (3, 3))\n",
    "\n",
    "    def forward(self, quat):\n",
    "        return self.quaternion_to_matrix(quat)\n",
    "        # # ori, calibration_quats: (..., 4)  quats in (x, y, z, w) order\n",
    "        # # acc:                   (..., 3)\n",
    "        # # device_id:             (1,)  – choose which calibration quat to use\n",
    "\n",
    "        # device_mean_quat = calibration_quats     # stays a tensor\n",
    "\n",
    "        # og_mat   = self.quaternion_to_matrix(ori)                   # (..., 3, 3)\n",
    "        # global_if = self.quaternion_to_matrix(device_mean_quat)     # (3, 3)\n",
    "        # return og_mat, global_if\n",
    "\n",
    "        # global_mat = torch.matmul(global_if.T, og_mat)         # R_g←s\n",
    "        # global_ori = matrix_to_quaternion(global_mat)          # (..., 4)\n",
    "\n",
    "        # acc_ref   = torch.matmul(og_mat, acc.unsqueeze(-1)).squeeze(-1)\n",
    "        # global_acc = torch.matmul(global_if.T, acc_ref.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # return global_ori, global_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27cab13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quat2matrix_func = Quaternion2Matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b11f1aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_input = torch.ones((4))\n",
    "acc_input = torch.ones((3))\n",
    "calibration_quat = torch.ones((4))\n",
    "traced_func = torch.jit.trace(quat2matrix_func, example_inputs=(ori_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e52ca95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not in eval mode. Consider calling '.eval()' on your model prior to conversion\n",
      "Converting PyTorch Frontend ==> MIL Ops:  99%|█████████▊| 72/73 [00:00<00:00, 3658.53 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 1108.67 passes/s]\n",
      "Running MIL default pipeline:   0%|          | 0/89 [00:00<?, ? passes/s]/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '77', of the source model, has been renamed to 'var_77' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL default pipeline: 100%|██████████| 89/89 [00:00<00:00, 665.85 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 1553.40 passes/s]\n"
     ]
    }
   ],
   "source": [
    "quat2matrix_func_from_trace = ct.convert(\n",
    "    traced_func,\n",
    "    inputs=[ct.TensorType(shape=ori_input.shape)],\n",
    "    convert_to=\"mlprogram\")\n",
    "quat2matrix_func_from_trace.save(\"Quat2Matrix.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92f87b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.transforms import quaternion_to_matrix, matrix_to_quaternion, standardize_quaternion\n",
    "\n",
    "class Sensor2Global(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def quaternion_to_matrix(self, quaternions: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        r, i, j, k = torch.unbind(quaternions, -1)\n",
    "        # pyre-fixme[58]: `/` is not supported for operand types `float` and `Tensor`.\n",
    "        two_s = 2.0 / (quaternions * quaternions).sum(-1)\n",
    "\n",
    "        o = torch.stack(\n",
    "            (\n",
    "                1 - two_s * (j * j + k * k),\n",
    "                two_s * (i * j - k * r),\n",
    "                two_s * (i * k + j * r),\n",
    "                two_s * (i * j + k * r),\n",
    "                1 - two_s * (i * i + k * k),\n",
    "                two_s * (j * k - i * r),\n",
    "                two_s * (i * k - j * r),\n",
    "                two_s * (j * k + i * r),\n",
    "                1 - two_s * (i * i + j * j),\n",
    "            ),\n",
    "            -1,\n",
    "        )\n",
    "        return o.reshape(quaternions.shape[:-1] + (3, 3))\n",
    "    \n",
    "\n",
    "    def forward(self, ori, acc, calibration_quats):\n",
    "        # # ori, calibration_quats: (..., 4)  quats in (x, y, z, w) order\n",
    "        # # acc:                   (..., 3)\n",
    "        # # device_id:             (1,)  – choose which calibration quat to use\n",
    "\n",
    "        device_mean_quat = calibration_quats     # stays a tensor\n",
    "\n",
    "        og_mat   = self.quaternion_to_matrix(ori)                   # (..., 3, 3)\n",
    "        global_if = self.quaternion_to_matrix(device_mean_quat)     # (3, 3)\n",
    "\n",
    "        global_mat = torch.matmul(global_if.T, og_mat)         # R_g←s\n",
    "        global_ori = matrix_to_quaternion_single(global_mat)          # (..., 4)\n",
    "        acc_ref   = torch.matmul(og_mat, acc.unsqueeze(-1)).squeeze(-1)\n",
    "        global_acc = torch.matmul(global_if.T, acc_ref.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        return global_ori, global_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80c01351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sensor2Global()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor2global_func = Sensor2Global()\n",
    "sensor2global_func.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba073dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting PyTorch Frontend ==> MIL Ops:  99%|█████████▉| 336/338 [00:00<00:00, 5293.61 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 277.02 passes/s]\n",
      "Running MIL default pipeline:   0%|          | 0/89 [00:00<?, ? passes/s]/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '335', of the source model, has been renamed to 'var_335' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '346', of the source model, has been renamed to 'var_346' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL default pipeline: 100%|██████████| 89/89 [00:00<00:00, 152.30 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 284.24 passes/s]\n"
     ]
    }
   ],
   "source": [
    "traced_func = torch.jit.trace(sensor2global_func, example_inputs=(ori_input, acc_input, calibration_quat))\n",
    "sensor2global_func_from_trace = ct.convert(\n",
    "    traced_func,\n",
    "    inputs=[ct.TensorType(shape=ori_input.shape), ct.TensorType(shape=acc_input.shape), ct.TensorType(shape=calibration_quat.shape)],\n",
    "    convert_to=\"mlprogram\")\n",
    "quat2matrix_func_from_trace.save(\"Sensor2Global.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97907727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def matrix_to_quaternion_single(matrix: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args\n",
    "    ----\n",
    "    matrix : torch.Tensor, shape (3, 3)\n",
    "             Rotation matrix  (rows are destination axes).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    quat   : torch.Tensor, shape (4,)  --  (w, x, y, z) with ||quat|| == 1\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------\n",
    "    # 1.  Pull out the nine elements\n",
    "    # ------------------------------\n",
    "    m00, m01, m02 = matrix[0, 0], matrix[0, 1], matrix[0, 2]\n",
    "    m10, m11, m12 = matrix[1, 0], matrix[1, 1], matrix[1, 2]\n",
    "    m20, m21, m22 = matrix[2, 0], matrix[2, 1], matrix[2, 2]\n",
    "\n",
    "    one = matrix.new_tensor(1.0)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 2.  Same four “absolute” terms as the original function\n",
    "    # -------------------------------------------------------\n",
    "    q_abs = torch.sqrt(\n",
    "        torch.clamp(\n",
    "            torch.stack(\n",
    "                [\n",
    "                    one + m00 + m11 + m22,        # w‑candidate\n",
    "                    one + m00 - m11 - m22,        # x‑candidate\n",
    "                    one - m00 + m11 - m22,        # y‑candidate\n",
    "                    one - m00 - m11 + m22,        # z‑candidate\n",
    "                ]\n",
    "            ),\n",
    "            min=0.0,\n",
    "        )\n",
    "    )                                           # shape (4,)\n",
    "\n",
    "    quat_by_rijk = torch.stack(\n",
    "        [\n",
    "            torch.stack([q_abs[0] ** 2,\n",
    "                         m21 - m12,\n",
    "                         m02 - m20,\n",
    "                         m10 - m01]),\n",
    "            torch.stack([m21 - m12,\n",
    "                         q_abs[1] ** 2,\n",
    "                         m10 + m01,\n",
    "                         m02 + m20]),\n",
    "            torch.stack([m02 - m20,\n",
    "                         m10 + m01,\n",
    "                         q_abs[2] ** 2,\n",
    "                         m12 + m21]),\n",
    "            torch.stack([m10 - m01,\n",
    "                         m20 + m02,\n",
    "                         m21 + m12,\n",
    "                         q_abs[3] ** 2]),\n",
    "        ],\n",
    "        dim=0,                                   # shape (4, 4)\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 4.  Divide each row by the same “safe” denominator\n",
    "    # ---------------------------------------------------\n",
    "    floor = matrix.new_tensor(0.1)\n",
    "    denom = 2.0 * torch.max(q_abs, floor)        # shape (4,)\n",
    "    quat_candidates = quat_by_rijk / denom.unsqueeze(-1)\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 5.  Pick the best‑conditioned row (largest q_abs element)\n",
    "    #     – use tensor‑indexing so the tracer records a gather\n",
    "    # ----------------------------------------------------------\n",
    "    best_idx = torch.argmax(q_abs)               # tensor scalar\n",
    "    quat = quat_candidates[best_idx]             # shape (4,)\n",
    "\n",
    "    # 6.  Normalize to exactly unit length and return (w, x, y, z)\n",
    "    return quat / quat.norm(p=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b482478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from typing import List\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ProcessInputs(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, \n",
    "                imu: torch.Tensor, \n",
    "                ori_raw: torch.Tensor, \n",
    "                acc_raw: torch.Tensor, \n",
    "                acc_offsets: torch.Tensor, \n",
    "                smpl2imu: torch.Tensor, \n",
    "                device2bone: torch.Tensor) -> torch.Tensor:\n",
    "        ori_raw = quaternion_to_rotation_matrix(ori_raw).view(-1, 5, 3, 3) # hardcoded n_imus = 5\n",
    "        glb_acc = (smpl2imu.matmul(acc_raw.view(-1, 5, 3, 1)) - acc_offsets).view(-1, 5, 3) # hardcoded n_imus = 5\n",
    "        glb_ori = smpl2imu.matmul(ori_raw).matmul(device2bone)\n",
    "\n",
    "        # normalization \n",
    "        _acc = glb_acc.view(-1, 5, 3)[:, [1, 4, 3, 0, 2]] / 30 #hardcoded acc_scale = 30\n",
    "        _ori = glb_ori.view(-1, 5, 3, 3)[:, [1, 4, 3, 0, 2]]\n",
    "\n",
    "        acc = torch.zeros_like(_acc)\n",
    "        ori = torch.zeros_like(_ori)\n",
    "\n",
    "        # device combo\n",
    "        # c = [1, 3] # hardcoded rw_rp': [1, 3]\n",
    "        c = [3] #hardcoded rp: [3]\n",
    "\n",
    "        acc[:, c] = _acc[:, c] \n",
    "        ori[:, c] = _ori[:, c]\n",
    "        \n",
    "        imu_input = torch.cat([acc.flatten(1), ori.flatten(1)], dim=1).squeeze(0)\n",
    "\n",
    "        # Pushinng this logical if statement to Swift\n",
    "        # imu_input.repeat(45, 1) if imu is None else \n",
    "        imu = torch.cat((imu[1:], imu_input.view(1, -1))) #hardcoded num_total_frames = 45\n",
    "\n",
    "        return imu.unsqueeze(0), torch.tensor([45]), imu.squeeze(0) #hardcoded num_total_frames = 45\n",
    "        # imu_input, imu_shape, self.imu to store\n",
    "\n",
    "\n",
    "class ProcessInputsInitial(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,  \n",
    "                ori_raw: torch.Tensor, \n",
    "                acc_raw: torch.Tensor, \n",
    "                acc_offsets: torch.Tensor, \n",
    "                smpl2imu: torch.Tensor, \n",
    "                device2bone: torch.Tensor) -> torch.Tensor:\n",
    "        ori_raw = quaternion_to_rotation_matrix(ori_raw).view(-1, 5, 3, 3) # hardcoded n_imus = 5\n",
    "        glb_acc = (smpl2imu.matmul(acc_raw.view(-1, 5, 3, 1)) - acc_offsets).view(-1, 5, 3) # hardcoded n_imus = 5\n",
    "        glb_ori = smpl2imu.matmul(ori_raw).matmul(device2bone)\n",
    "\n",
    "        # normalization \n",
    "        _acc = glb_acc.view(-1, 5, 3)[:, [1, 4, 3, 0, 2]] / 30 #hardcoded acc_scale = 30\n",
    "        _ori = glb_ori.view(-1, 5, 3, 3)[:, [1, 4, 3, 0, 2]]\n",
    "\n",
    "        acc = torch.zeros_like(_acc)\n",
    "        ori = torch.zeros_like(_ori)\n",
    "\n",
    "        # device combo\n",
    "        # c = [1, 3] # hardcoded rw_rp': [1, 3]\n",
    "        c = [3] #hardcoded rp: [3]\n",
    "\n",
    "        acc[:, c] = _acc[:, c] \n",
    "        ori[:, c] = _ori[:, c]\n",
    "        \n",
    "        imu_input = torch.cat([acc.flatten(1), ori.flatten(1)], dim=1).squeeze(0)\n",
    "\n",
    "        # Pushinng this logical if statement to Swift\n",
    "        imu = imu_input.repeat(45, 1)\n",
    "\n",
    "        return imu.unsqueeze(0), torch.tensor([45]), imu.squeeze(0) #hardcoded num_total_frames = 45\n",
    "        # imu_input, imu_shape, self.imu to store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1b6b71",
   "metadata": {},
   "source": [
    "Orientation_shape: torch.Size([1, 5, 4]) \\\n",
    "Acceleration_shape: torch.Size([1, 5, 3]) \\\n",
    "SMPL2IMU_shape: torch.Size([3, 3]) \\\n",
    "AccOffset_shape: torch.Size([5, 3, 1]) \\\n",
    "Device2Bone_shape: torch.Size([5, 3, 3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7726fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imu_frames = torch.rand_like(torch.zeros((45, 60)))\n",
    "ori_raw = torch.rand_like(torch.zeros((1, 5, 4)))\n",
    "acc_raw = torch.rand_like(torch.zeros((1, 5, 3)))\n",
    "smpl2imu = torch.rand_like(torch.zeros((3, 3)))\n",
    "accOffset = torch.rand_like(torch.zeros((5, 3, 1)))\n",
    "device2bone = torch.rand_like(torch.zeros((5, 3, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a971a453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nx/23kzl3_d08d89039y9hptpf40000gn/T/ipykernel_64404/1743620849.py:44: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return imu.unsqueeze(0), torch.tensor([45]), imu.squeeze(0) #hardcoded num_total_frames = 45\n",
      "Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting PyTorch Frontend ==> MIL Ops:  99%|█████████▉| 320/322 [00:00<00:00, 5726.23 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 997.60 passes/s]\n",
      "Running MIL default pipeline:   0%|          | 0/89 [00:00<?, ? passes/s]/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:245: UserWarning: Input, 'imu.1', of the source model, has been renamed to 'imu_1' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:245: UserWarning: Input, 'ori_raw.1', of the source model, has been renamed to 'ori_raw_1' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '317', of the source model, has been renamed to 'var_317' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL default pipeline: 100%|██████████| 89/89 [00:00<00:00, 1712.00 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 4426.32 passes/s]\n",
      "/var/folders/nx/23kzl3_d08d89039y9hptpf40000gn/T/ipykernel_64404/1743620849.py:81: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return imu.unsqueeze(0), torch.tensor([45]), imu.squeeze(0) #hardcoded num_total_frames = 45\n",
      "Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting PyTorch Frontend ==> MIL Ops:  99%|█████████▉| 312/314 [00:00<00:00, 4404.85 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 1231.81 passes/s]\n",
      "Running MIL default pipeline:   0%|          | 0/89 [00:00<?, ? passes/s]/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '308', of the source model, has been renamed to 'var_308' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL default pipeline: 100%|██████████| 89/89 [00:00<00:00, 2257.46 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 8633.22 passes/s]\n"
     ]
    }
   ],
   "source": [
    "process_func = ProcessInputs()\n",
    "process_func.eval()\n",
    "process_func_initial = ProcessInputsInitial()\n",
    "process_func_initial.eval()\n",
    "\n",
    "traced_func = torch.jit.trace(process_func, example_inputs=(imu_frames, ori_raw, acc_raw, accOffset, smpl2imu, device2bone))\n",
    "process_func_model = ct.convert(\n",
    "    traced_func,\n",
    "    inputs=[ct.TensorType(shape=imu_frames.shape), \n",
    "            ct.TensorType(shape=ori_raw.shape), \n",
    "            ct.TensorType(shape=acc_raw.shape), \n",
    "            ct.TensorType(shape=accOffset.shape),\n",
    "            ct.TensorType(shape=smpl2imu.shape), \n",
    "            ct.TensorType(shape=device2bone.shape)],\n",
    "    convert_to=\"mlprogram\")\n",
    "process_func_model.save(\"ProcessInputs.mlpackage\")\n",
    "\n",
    "traced_func = torch.jit.trace(process_func_initial, example_inputs=(ori_raw, acc_raw, accOffset, smpl2imu, device2bone))\n",
    "process_func_model_initial = ct.convert(\n",
    "    traced_func,\n",
    "    inputs=[ct.TensorType(shape=ori_raw.shape), \n",
    "            ct.TensorType(shape=acc_raw.shape), \n",
    "            ct.TensorType(shape=accOffset.shape),\n",
    "            ct.TensorType(shape=smpl2imu.shape), \n",
    "            ct.TensorType(shape=device2bone.shape)],\n",
    "    convert_to=\"mlprogram\")\n",
    "process_func_model_initial.save(\"ProcessInputsInitial.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99588d92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobileposer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
