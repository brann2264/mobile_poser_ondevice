{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3454c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import socket\n",
    "import threading\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from argparse import ArgumentParser\n",
    "os.environ['PYGAME_HIDE_SUPPORT_PROMPT'] = \"hide\"\n",
    "from pygame.time import Clock\n",
    "import pickle\n",
    "\n",
    "from articulate.math import *\n",
    "from mobileposer.models import *\n",
    "from mobileposer.utils.model_utils import *\n",
    "from mobileposer.config import *\n",
    "\n",
    "import coremltools as ct\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53cd0b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mobileposer.models.rnn import RNN\n",
    "class JointsBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs: N IMUs.\n",
    "    Outputs: 24 Joint positions. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_imu, seq_length):\n",
    "        super().__init__()\n",
    "        # self.joints = net.joints.joints\n",
    "        self.joints = RNN(n_imu, 24 * 3, 256, seq_length)\n",
    "\n",
    "    def forward(self, batch, input_lengths: Tensor):\n",
    "        # forward joint model\n",
    "        joints, _, _ = self.joints(batch)\n",
    "        return joints\n",
    "\n",
    "class PoserBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs: N IMUs.\n",
    "    Outputs: SMPL Pose Parameters (as 6D Rotations).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_output_joints, n_imu, n_reduced, seq_length):\n",
    "        super().__init__()\n",
    "        # self.pose = net.pose.pose\n",
    "        self.pose = RNN(n_output_joints*3 + n_imu, n_reduced*6, 256, seq_length)\n",
    "\n",
    "    def forward(self, batch, input_lengths: Tensor):\n",
    "        # forward the pose prediction model\n",
    "        pred_pose, _, _ = self.pose(batch)\n",
    "        return pred_pose\n",
    "\n",
    "class VelocityBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs: N IMUs.\n",
    "    Outputs: Per-Frame Root Velocity. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_output_joints, n_imu, seq_length):\n",
    "        super().__init__()\n",
    "\n",
    "        # model definitions\n",
    "        # self.vel = net.velocity.vel\n",
    "        self.vel = RNN(n_output_joints * 3 + n_imu, 24 * 3, 256, bidirectional=False, seq_length=seq_length)\n",
    "\n",
    "    def forward(self, batch, h, c, input_lengths:Tensor):\n",
    "        # forward velocity model\n",
    "        vel, _, state = self.vel(batch, (h,c))\n",
    "        h_out, c_out = state[0].detach(), state[1].detach()\n",
    "        return vel, h_out, c_out\n",
    "    \n",
    "class FootContactBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs: N IMUs.\n",
    "    Outputs: Foot Contact Probability ([s_lfoot, s_rfoot]).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_output_joints, n_imu, seq_length):\n",
    "        super().__init__()\n",
    "        # self.footcontact = net.foot_contact.footcontact\n",
    "        self.footcontact = RNN(n_output_joints * 3 + n_imu, 2, 64, seq_length=seq_length)\n",
    "\n",
    "    def forward(self, batch, input_lengths: Tensor):\n",
    "        # forward foot contact model\n",
    "        foot_contact, _, _ = self.footcontact(batch)\n",
    "        return foot_contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7c694c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobilePoserBase(nn.Module):\n",
    "    # def __init__(self, net, body_model, joints_model, pose_model, contact_model, velocity_model, n_reduced, ignored):\n",
    "    #     super().__init__()\n",
    "\n",
    "    #     #constants\n",
    "    #     self.n_reduced = n_reduced\n",
    "    #     self.ignored = ignored\n",
    "\n",
    "    #     #core model layers\n",
    "    #     self.bodymodel = body_model\n",
    "    #     self.joints = joints_model\n",
    "    #     self.pose = pose_model\n",
    "    #     self.foot_contact = contact_model\n",
    "    #     self.velocity = velocity_model\n",
    "    def __init__(self, n_reduced, ignored, n_imu, n_output_joints, seq_length):\n",
    "        super().__init__()\n",
    "\n",
    "        #constants\n",
    "        self.n_reduced = n_reduced\n",
    "        self.ignored = ignored\n",
    "\n",
    "        #core model layers\n",
    "        self.joints = JointsBase(n_imu=n_imu, seq_length=seq_length)\n",
    "        self.pose = PoserBase(n_imu=n_imu, n_output_joints=n_output_joints, n_reduced=n_reduced, seq_length=seq_length)\n",
    "        self.foot_contact = FootContactBase(n_output_joints=n_output_joints, n_imu=n_imu, seq_length=seq_length)\n",
    "        self.velocity = VelocityBase(n_imu=n_imu, n_output_joints=n_output_joints, seq_length=seq_length)\n",
    "    \n",
    "    def normalize_tensor(self, x: torch.Tensor, dim: int=-1):\n",
    "        norm = torch.norm(x, dim=dim, keepdim=True)\n",
    "        normalized_x = x / norm\n",
    "        return normalized_x\n",
    "    \n",
    "    def quaternion_to_rotation_matrix(self, q: torch.Tensor):\n",
    "        q = self.normalize_tensor(q.view(-1, 4))\n",
    "        a, b, c, d = q[:, 0:1], q[:, 1:2], q[:, 2:3], q[:, 3:4]\n",
    "        r = torch.cat((- 2 * c * c - 2 * d * d + 1, 2 * b * c - 2 * a * d, 2 * a * c + 2 * b * d,\n",
    "                    2 * b * c + 2 * a * d, - 2 * b * b - 2 * d * d + 1, 2 * c * d - 2 * a * b,\n",
    "                    2 * b * d - 2 * a * c, 2 * a * b + 2 * c * d, - 2 * b * b - 2 * c * c + 1), dim=1)\n",
    "        return r.view(-1, 3, 3)\n",
    "    \n",
    "    def forward(self, \n",
    "                # imu: torch.Tensor, \n",
    "                ori_raw: torch.Tensor, \n",
    "                acc_raw: torch.Tensor, \n",
    "                acc_offsets: torch.Tensor, \n",
    "                smpl2imu: torch.Tensor, \n",
    "                device2bone: torch.Tensor,\n",
    "                h: torch.Tensor,\n",
    "                c: torch.Tensor):\n",
    "        ori_raw = self.quaternion_to_rotation_matrix(ori_raw).view(-1, 5, 3, 3) # hardcoded n_imus = 5\n",
    "        glb_acc = (smpl2imu.matmul(acc_raw.view(-1, 5, 3, 1)) - acc_offsets).view(-1, 5, 3) # hardcoded n_imus = 5\n",
    "        glb_ori = smpl2imu.matmul(ori_raw).matmul(device2bone)\n",
    "\n",
    "        # normalization \n",
    "        _acc = glb_acc.view(-1, 5, 3)[:, [1, 4, 3, 0, 2]] / 30 #hardcoded acc_scale = 30\n",
    "        _ori = glb_ori.view(-1, 5, 3, 3)[:, [1, 4, 3, 0, 2]]\n",
    "\n",
    "        acc = torch.zeros_like(_acc)\n",
    "        ori = torch.zeros_like(_ori)\n",
    "\n",
    "        # device combo\n",
    "        # c = [1, 3] # hardcoded rw_rp': [1, 3]\n",
    "        # combo = torch.tensor([3], dtype=torch.long) #hardcoded rp: [3]\n",
    "\n",
    "        mask_1d = torch.tensor([0, 0, 0, 1, 0], dtype=acc_raw.dtype)  \n",
    "        mask_1d = mask_1d.view(1, 5)                           \n",
    "        mask_acc = mask_1d.unsqueeze(-1)                # [1,5,1]\n",
    "        mask_ori = mask_1d.view(1, 5, 1, 1)\n",
    "\n",
    "        acc = _acc * mask_acc      # [1,5,3], all zeros except channel 3\n",
    "        ori = _ori * mask_ori      # [1,5,3,3], all zeros except channel 3\n",
    "             \n",
    "        imu_input = torch.cat([acc.flatten(1), ori.flatten(1)], dim=1).squeeze(0)\n",
    "\n",
    "        # Pushinng this logical if statement to Swift\n",
    "        imu = imu_input.repeat(45, 1)\n",
    "        # imu = torch.cat((imu[1:], imu_input.view(1, -1))) #hardcoded num_total_frames = 45\n",
    "\n",
    "        pred_pose, pred_joints, pred_vel, foot_contact, velocity_h, velocity_c = self.run_model(imu.unsqueeze(0), input_lengths=torch.tensor([45]), h=h, c=c)\n",
    "\n",
    "        return pred_pose, pred_joints, pred_vel, foot_contact, velocity_h, velocity_c, imu.squeeze(0)\n",
    "        # imu to store\n",
    "\n",
    "    def run_model(self, \n",
    "                  batch: torch.Tensor, \n",
    "                  h: torch.Tensor, \n",
    "                  c: torch.Tensor, \n",
    "                  input_lengths: torch.Tensor):\n",
    "        # forward the joint prediction model\n",
    "        pred_joints = self.joints(batch, input_lengths)\n",
    "\n",
    "        # forward the pose prediction model\n",
    "        pose_input = torch.cat((pred_joints, batch), dim=-1)\n",
    "        pred_pose = self.pose(pose_input, input_lengths)\n",
    "\n",
    "        # forward the foot-ground contact probability model\n",
    "        tran_input = torch.cat((pred_joints, batch), dim=-1)\n",
    "        foot_contact = self.foot_contact(tran_input, input_lengths)\n",
    "\n",
    "        # foward the foot-joint velocity model\n",
    "        pred_vel, velocity_h, velocity_c = self.velocity(tran_input, h, c, input_lengths)\n",
    "        pred_vel = pred_vel.squeeze(0)\n",
    "\n",
    "        pred_pose, pred_joints, pred_vel, foot_contact = self.process_base_outputs(pred_pose, pred_joints, pred_vel, foot_contact)\n",
    "\n",
    "        return pred_pose, pred_joints, pred_vel, foot_contact, velocity_h, velocity_c\n",
    "    \n",
    "    def rotation_matrix_to_axis_angle(self, r: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param r: Tensor of shape (..., 3, 3), a batch of rotation matrices\n",
    "        :return: Tensor of shape (..., 3), the corresponding axis-angle vectors\n",
    "        \"\"\"\n",
    "        # Flatten batch dims\n",
    "        R = r.view(-1, 3, 3)\n",
    "\n",
    "        # 1) compute the trace → cos θ\n",
    "        tr = R[..., 0, 0] + R[..., 1, 1] + R[..., 2, 2]\n",
    "        cos_theta = (tr - 1.0) * 0.5\n",
    "        cos_theta = cos_theta.clamp(-1.0, 1.0)\n",
    "\n",
    "        # 2) recover θ\n",
    "        theta = torch.acos(cos_theta)\n",
    "\n",
    "        # 3) compute the \"cross-differences\" v = [R32-R23, R13-R31, R21-R12]\n",
    "        rx = R[..., 2, 1] - R[..., 1, 2]\n",
    "        ry = R[..., 0, 2] - R[..., 2, 0]\n",
    "        rz = R[..., 1, 0] - R[..., 0, 1]\n",
    "        v   = torch.stack((rx, ry, rz), dim=-1)\n",
    "\n",
    "        # 4) normalize to get the rotation axis: axis = v / (2 sin θ)\n",
    "        sin_theta = torch.sin(theta).clamp(min=1e-6).unsqueeze(-1)\n",
    "        axis = v / (2.0 * sin_theta)\n",
    "\n",
    "        # 5) axis-angle vector = axis * θ\n",
    "        rot_vec = axis * theta.unsqueeze(-1)\n",
    "\n",
    "        # return rot_vec.view(R.shape[0], 3)\n",
    "        return rot_vec.flatten()\n",
    "    \n",
    "    def process_base_outputs(self, pose, pred_joints, vel, contact):\n",
    "        \n",
    "        pose = art.math.r6d_to_rotation_matrix(pose).reshape(-1, 24, 3, 3)\n",
    "        \n",
    "        # get pose\n",
    "        curr_pose = pose[40]   # shape = [24, 3, 3]\n",
    "        curr_pose = curr_pose.flatten(start_dim=2)\n",
    "        curr_pose = self.rotation_matrix_to_axis_angle(curr_pose)\n",
    "\n",
    "        # compute the joint positions from predicted pose\n",
    "        joints = pred_joints.squeeze(0)[40].view(24, 3)\n",
    "\n",
    "        # compute translation from foot-contact probability\n",
    "        contact = contact[0][40]\n",
    "        \n",
    "        # velocity from network-based estimation\n",
    "        root_vel = vel.view(-1, 24, 3)[:, 0]\n",
    "\n",
    "        pred_vel = root_vel[40] / (30/2) #hardcoded fps = 30, vel_scale = 2\n",
    "\n",
    "        # Need to implement in Swift\n",
    "\n",
    "        # lfoot_pos, rfoot_pos = joints[10], joints[11]\n",
    "        # if contact[0] > contact[1]:\n",
    "        #     contact_vel = self.last_lfoot_pos - lfoot_pos + self.gravity_velocity\n",
    "        # else:\n",
    "        #     contact_vel = self.last_rfoot_pos - rfoot_pos + self.gravity_velocity\n",
    "        # weight = self._prob_to_weight(contact.max())\n",
    "        # velocity = art.math.lerp(pred_vel, contact_vel, weight)\n",
    "        # current_foot_y = self.current_root_y + min(lfoot_pos[1].item(), rfoot_pos[1].item())\n",
    "        # if current_foot_y + velocity[1].item() <= self.floor_y:\n",
    "        #     velocity[1] = self.floor_y - current_foot_y\n",
    "\n",
    "        # self.current_root_y += velocity[1].item()\n",
    "        # self.last_lfoot_pos, self.last_rfoot_pos = lfoot_pos, rfoot_pos\n",
    "        # self.last_root_pos += velocity\n",
    "\n",
    "        return curr_pose, joints, pred_vel, contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "166630d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobilePoserBase(\n",
    "                    n_reduced=joint_set.n_full,\n",
    "                    # n_reduced=joint_set.n_reduced,\n",
    "                    ignored=joint_set.ignored,\n",
    "                    n_imu=model_config.n_imu,\n",
    "                    n_output_joints=model_config.n_output_joints,\n",
    "                    seq_length=torch.tensor([model_config.past_frames+model_config.future_frames]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d8653c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobilePoserBase(\n",
       "  (joints): JointsBase(\n",
       "    (joints): RNN(\n",
       "      (rnn): LSTM(256, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "      (linear1): Linear(in_features=60, out_features=256, bias=True)\n",
       "      (linear2): Linear(in_features=512, out_features=72, bias=True)\n",
       "      (dropout): Dropout(p=0.4, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (pose): PoserBase(\n",
       "    (pose): RNN(\n",
       "      (rnn): LSTM(256, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "      (linear1): Linear(in_features=132, out_features=256, bias=True)\n",
       "      (linear2): Linear(in_features=512, out_features=144, bias=True)\n",
       "      (dropout): Dropout(p=0.4, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (foot_contact): FootContactBase(\n",
       "    (footcontact): RNN(\n",
       "      (rnn): LSTM(64, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       "      (linear1): Linear(in_features=132, out_features=64, bias=True)\n",
       "      (linear2): Linear(in_features=128, out_features=2, bias=True)\n",
       "      (dropout): Dropout(p=0.4, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (velocity): VelocityBase(\n",
       "    (vel): RNN(\n",
       "      (rnn): LSTM(256, 256, num_layers=2, batch_first=True)\n",
       "      (linear1): Linear(in_features=132, out_features=256, bias=True)\n",
       "      (linear2): Linear(in_features=256, out_features=72, bias=True)\n",
       "      (dropout): Dropout(p=0.4, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/Users/brianchen/Research/MobilePoser/mobileposer/checkpoints/model_finetuned.pth\", map_location=\"cpu\"))\n",
    "# base.load_state_dict(torch.load(\"/Users/brianchen/Research/MobilePoser/mobileposer/checkpoints/weights.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "784b4069",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_model = torch.jit.script(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a440acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_past_frames = model_config.past_frames\n",
    "num_future_frames = model_config.future_frames\n",
    "num_total_frames = num_past_frames + num_future_frames\n",
    "data = torch.zeros((60))\n",
    "imu = data.repeat(num_total_frames, 1)\n",
    "\n",
    "input_length = torch.tensor([num_total_frames])\n",
    "imu_input = imu.unsqueeze(0)\n",
    "h, c = (torch.zeros((2, 1, 256)), torch.zeros((2, 1, 256)))\n",
    "imu_frames = torch.rand_like(torch.zeros((45, 60)))\n",
    "ori_raw = torch.rand_like(torch.zeros((1, 5, 4)))\n",
    "acc_raw = torch.rand_like(torch.zeros((1, 5, 3)))\n",
    "smpl2imu = torch.rand_like(torch.zeros((3, 3)))\n",
    "accOffset = torch.rand_like(torch.zeros((5, 3, 1)))\n",
    "device2bone = torch.rand_like(torch.zeros((5, 3, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d5d36ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(\n",
    "                # imu_frames, \n",
    "                ori_raw, \n",
    "                acc_raw,\n",
    "                accOffset,\n",
    "                smpl2imu,\n",
    "                device2bone,\n",
    "                h,\n",
    "                c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5507d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nx/23kzl3_d08d89039y9hptpf40000gn/T/ipykernel_72907/1806245280.py:65: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask_1d = torch.tensor([0, 0, 0, 1, 0], dtype=acc_raw.dtype)\n",
      "/var/folders/nx/23kzl3_d08d89039y9hptpf40000gn/T/ipykernel_72907/1806245280.py:79: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  pred_pose, pred_joints, pred_vel, foot_contact, velocity_h, velocity_c = self.run_model(imu.unsqueeze(0), input_lengths=torch.tensor([45]), h=h, c=c)\n",
      "When both 'convert_to' and 'minimum_deployment_target' not specified, 'convert_to' is set to \"mlprogram\" and 'minimum_deployment_target' is set to ct.target.iOS15 (which is same as ct.target.macOS12). Note: the model will not run on systems older than iOS15/macOS12/watchOS8/tvOS15. In order to make your model run on older system, please set the 'minimum_deployment_target' to iOS14/iOS13. Details please see the link: https://apple.github.io/coremltools/docs-guides/source/target-conversion-formats.html\n",
      "Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 553/555 [00:00<00:00, 1491.99 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 76.30 passes/s]\n",
      "Running MIL default pipeline:   9%|▉         | 8/89 [00:00<00:01, 78.87 passes/s]/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:245: UserWarning: Input, 'ori_raw.1', of the source model, has been renamed to 'ori_raw_1' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:245: UserWarning: Input, 'c.1', of the source model, has been renamed to 'c_1' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '434', of the source model, has been renamed to 'var_434' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '435', of the source model, has been renamed to 'var_435' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '632', of the source model, has been renamed to 'var_632' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '641', of the source model, has been renamed to 'var_641' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '647', of the source model, has been renamed to 'var_647' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '665', of the source model, has been renamed to 'var_665' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL default pipeline: 100%|██████████| 89/89 [00:01<00:00, 63.90 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 167.36 passes/s]\n"
     ]
    }
   ],
   "source": [
    "scripted_core = scripted_model.eval()\n",
    "traced_core = torch.jit.trace(model, (\n",
    "                                                # imu_frames, \n",
    "                                                ori_raw, \n",
    "                                                acc_raw,\n",
    "                                                accOffset,\n",
    "                                                smpl2imu,\n",
    "                                                device2bone,\n",
    "                                                h,\n",
    "                                                c))\n",
    "model_from_trace = ct.convert(\n",
    "    traced_core,\n",
    "    inputs=[\n",
    "            # ct.TensorType(shape=imu_frames.shape), \n",
    "            ct.TensorType(shape=ori_raw.shape), \n",
    "            ct.TensorType(shape=acc_raw.shape), \n",
    "            ct.TensorType(shape=accOffset.shape),\n",
    "            ct.TensorType(shape=smpl2imu.shape),\n",
    "            ct.TensorType(shape=device2bone.shape),\n",
    "            ct.TensorType(shape=h.shape),\n",
    "            ct.TensorType(shape=c.shape)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1692e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_from_trace.save(\"MobilePoserCompleteInitial.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b482478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from typing import List\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ProcessInputs(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, \n",
    "                imu: torch.Tensor, \n",
    "                ori_raw: torch.Tensor, \n",
    "                acc_raw: torch.Tensor, \n",
    "                acc_offsets: torch.Tensor, \n",
    "                smpl2imu: torch.Tensor, \n",
    "                device2bone: torch.Tensor) -> torch.Tensor:\n",
    "        ori_raw = quaternion_to_rotation_matrix(ori_raw).view(-1, 5, 3, 3) # hardcoded n_imus = 5\n",
    "        glb_acc = (smpl2imu.matmul(acc_raw.view(-1, 5, 3, 1)) - acc_offsets).view(-1, 5, 3) # hardcoded n_imus = 5\n",
    "        glb_ori = smpl2imu.matmul(ori_raw).matmul(device2bone)\n",
    "\n",
    "        # normalization \n",
    "        _acc = glb_acc.view(-1, 5, 3)[:, [1, 4, 3, 0, 2]] / 30 #hardcoded acc_scale = 30\n",
    "        _ori = glb_ori.view(-1, 5, 3, 3)[:, [1, 4, 3, 0, 2]]\n",
    "\n",
    "        acc = torch.zeros_like(_acc)\n",
    "        ori = torch.zeros_like(_ori)\n",
    "\n",
    "        # device combo\n",
    "        # c = [1, 3] # hardcoded rw_rp': [1, 3]\n",
    "        c = [3] #hardcoded rp: [3]\n",
    "\n",
    "        acc[:, c] = _acc[:, c] \n",
    "        ori[:, c] = _ori[:, c]\n",
    "        \n",
    "        imu_input = torch.cat([acc.flatten(1), ori.flatten(1)], dim=1).squeeze(0)\n",
    "\n",
    "        # Pushinng this logical if statement to Swift\n",
    "        # imu_input.repeat(45, 1) if imu is None else \n",
    "        imu = torch.cat((imu[1:], imu_input.view(1, -1))) #hardcoded num_total_frames = 45\n",
    "\n",
    "        return imu.unsqueeze(0), torch.tensor([45]), imu.squeeze(0) #hardcoded num_total_frames = 45\n",
    "        # imu_input, imu_shape, self.imu to store\n",
    "\n",
    "\n",
    "class ProcessInputsInitial(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,  \n",
    "                ori_raw: torch.Tensor, \n",
    "                acc_raw: torch.Tensor, \n",
    "                acc_offsets: torch.Tensor, \n",
    "                smpl2imu: torch.Tensor, \n",
    "                device2bone: torch.Tensor) -> torch.Tensor:\n",
    "        ori_raw = quaternion_to_rotation_matrix(ori_raw).view(-1, 5, 3, 3) # hardcoded n_imus = 5\n",
    "        glb_acc = (smpl2imu.matmul(acc_raw.view(-1, 5, 3, 1)) - acc_offsets).view(-1, 5, 3) # hardcoded n_imus = 5\n",
    "        glb_ori = smpl2imu.matmul(ori_raw).matmul(device2bone)\n",
    "\n",
    "        # normalization \n",
    "        _acc = glb_acc.view(-1, 5, 3)[:, [1, 4, 3, 0, 2]] / 30 #hardcoded acc_scale = 30\n",
    "        _ori = glb_ori.view(-1, 5, 3, 3)[:, [1, 4, 3, 0, 2]]\n",
    "\n",
    "        acc = torch.zeros_like(_acc)\n",
    "        ori = torch.zeros_like(_ori)\n",
    "\n",
    "        # device combo\n",
    "        # c = [1, 3] # hardcoded rw_rp': [1, 3]\n",
    "        c = [3] #hardcoded rp: [3]\n",
    "\n",
    "        acc[:, c] = _acc[:, c] \n",
    "        ori[:, c] = _ori[:, c]\n",
    "        \n",
    "        imu_input = torch.cat([acc.flatten(1), ori.flatten(1)], dim=1).squeeze(0)\n",
    "\n",
    "        # Pushinng this logical if statement to Swift\n",
    "        imu = imu_input.repeat(45, 1)\n",
    "\n",
    "        return imu.unsqueeze(0), torch.tensor([45]), imu.squeeze(0) #hardcoded num_total_frames = 45\n",
    "        # imu_input, imu_shape, self.imu to store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1b6b71",
   "metadata": {},
   "source": [
    "Orientation_shape: torch.Size([1, 5, 4]) \\\n",
    "Acceleration_shape: torch.Size([1, 5, 3]) \\\n",
    "SMPL2IMU_shape: torch.Size([3, 3]) \\\n",
    "AccOffset_shape: torch.Size([5, 3, 1]) \\\n",
    "Device2Bone_shape: torch.Size([5, 3, 3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7726fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imu_frames = torch.rand_like(torch.zeros((45, 60)))\n",
    "ori_raw = torch.rand_like(torch.zeros((1, 5, 4)))\n",
    "acc_raw = torch.rand_like(torch.zeros((1, 5, 3)))\n",
    "smpl2imu = torch.rand_like(torch.zeros((3, 3)))\n",
    "accOffset = torch.rand_like(torch.zeros((5, 3, 1)))\n",
    "device2bone = torch.rand_like(torch.zeros((5, 3, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a971a453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nx/23kzl3_d08d89039y9hptpf40000gn/T/ipykernel_64404/1743620849.py:44: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return imu.unsqueeze(0), torch.tensor([45]), imu.squeeze(0) #hardcoded num_total_frames = 45\n",
      "Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting PyTorch Frontend ==> MIL Ops:  99%|█████████▉| 320/322 [00:00<00:00, 5726.23 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 997.60 passes/s]\n",
      "Running MIL default pipeline:   0%|          | 0/89 [00:00<?, ? passes/s]/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:245: UserWarning: Input, 'imu.1', of the source model, has been renamed to 'imu_1' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:245: UserWarning: Input, 'ori_raw.1', of the source model, has been renamed to 'ori_raw_1' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '317', of the source model, has been renamed to 'var_317' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL default pipeline: 100%|██████████| 89/89 [00:00<00:00, 1712.00 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 4426.32 passes/s]\n",
      "/var/folders/nx/23kzl3_d08d89039y9hptpf40000gn/T/ipykernel_64404/1743620849.py:81: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return imu.unsqueeze(0), torch.tensor([45]), imu.squeeze(0) #hardcoded num_total_frames = 45\n",
      "Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting PyTorch Frontend ==> MIL Ops:  99%|█████████▉| 312/314 [00:00<00:00, 4404.85 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 1231.81 passes/s]\n",
      "Running MIL default pipeline:   0%|          | 0/89 [00:00<?, ? passes/s]/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '308', of the source model, has been renamed to 'var_308' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL default pipeline: 100%|██████████| 89/89 [00:00<00:00, 2257.46 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 8633.22 passes/s]\n"
     ]
    }
   ],
   "source": [
    "process_func = ProcessInputs()\n",
    "process_func.eval()\n",
    "process_func_initial = ProcessInputsInitial()\n",
    "process_func_initial.eval()\n",
    "\n",
    "traced_func = torch.jit.trace(process_func, example_inputs=(imu_frames, ori_raw, acc_raw, accOffset, smpl2imu, device2bone))\n",
    "process_func_model = ct.convert(\n",
    "    traced_func,\n",
    "    inputs=[ct.TensorType(shape=imu_frames.shape), \n",
    "            ct.TensorType(shape=ori_raw.shape), \n",
    "            ct.TensorType(shape=acc_raw.shape), \n",
    "            ct.TensorType(shape=accOffset.shape),\n",
    "            ct.TensorType(shape=smpl2imu.shape), \n",
    "            ct.TensorType(shape=device2bone.shape)],\n",
    "    convert_to=\"mlprogram\")\n",
    "process_func_model.save(\"ProcessInputs.mlpackage\")\n",
    "\n",
    "traced_func = torch.jit.trace(process_func_initial, example_inputs=(ori_raw, acc_raw, accOffset, smpl2imu, device2bone))\n",
    "process_func_model_initial = ct.convert(\n",
    "    traced_func,\n",
    "    inputs=[ct.TensorType(shape=ori_raw.shape), \n",
    "            ct.TensorType(shape=acc_raw.shape), \n",
    "            ct.TensorType(shape=accOffset.shape),\n",
    "            ct.TensorType(shape=smpl2imu.shape), \n",
    "            ct.TensorType(shape=device2bone.shape)],\n",
    "    convert_to=\"mlprogram\")\n",
    "process_func_model_initial.save(\"ProcessInputsInitial.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd27157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def quaternion_to_matrix(q: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert quaternion q=[x,y,z,w] (shape (...,4)) to a rotation matrix (...,3,3).\n",
    "    We use torch.stack on literal Python lists when forming each row.\n",
    "    \"\"\"\n",
    "    q = q.to(dtype=torch.float32)\n",
    "    q = q / q.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    x, y, z, w = q.unbind(-1)\n",
    "\n",
    "    xx = x*x; yy = y*y; zz = z*z; ww = w*w\n",
    "    xy = x*y; xz = x*z; xw = x*w\n",
    "    yz = y*z; yw = y*w; zw = z*w\n",
    "\n",
    "    m00 = ww + xx - yy - zz\n",
    "    m01 = 2*(xy - zw)\n",
    "    m02 = 2*(xz + yw)\n",
    "\n",
    "    m10 = 2*(xy + zw)\n",
    "    m11 = ww - xx + yy - zz\n",
    "    m12 = 2*(yz - xw)\n",
    "\n",
    "    m20 = 2*(xz - yw)\n",
    "    m21 = 2*(yz + xw)\n",
    "    m22 = ww - xx - yy + zz\n",
    "\n",
    "    row0 = torch.cat((m00.unsqueeze(-1), m01.unsqueeze(-1), m02.unsqueeze(-1)), dim=-1)  # shape (...,3)\n",
    "    row1 = torch.cat((m10.unsqueeze(-1), m11.unsqueeze(-1), m12.unsqueeze(-1)), dim=-1)\n",
    "    row2 = torch.cat((m20.unsqueeze(-1), m21.unsqueeze(-1), m22.unsqueeze(-1)), dim=-1)\n",
    "\n",
    "    return torch.cat(\n",
    "        (row0.unsqueeze(-2),  # shape (...,1,3)\n",
    "         row1.unsqueeze(-2),  # shape (...,1,3)\n",
    "         row2.unsqueeze(-2)), # shape (...,1,3)\n",
    "        dim=-2                # final shape (...,3,3)\n",
    "    )\n",
    "\n",
    "\n",
    "def matrix_to_quaternion(matrix: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert a valid rotation matrix (shape (...,3,3)) into a quaternion [x,y,z,w] of shape (...,4),\n",
    "    using only elementwise ops + torch.clamp + torch.sqrt + torch.sign + torch.stack.\n",
    "    This avoids any concat or boolean‐mask indexing.\n",
    "    \"\"\"\n",
    "    # 1) Unpack the 3×3 entries as scalars of shape (...)\n",
    "    m00 = matrix[..., 0, 0]\n",
    "    m01 = matrix[..., 0, 1]\n",
    "    m02 = matrix[..., 0, 2]\n",
    "    m10 = matrix[..., 1, 0]\n",
    "    m11 = matrix[..., 1, 1]\n",
    "    m12 = matrix[..., 1, 2]\n",
    "    m20 = matrix[..., 2, 0]\n",
    "    m21 = matrix[..., 2, 1]\n",
    "    m22 = matrix[..., 2, 2]\n",
    "\n",
    "    # 2) Compute trace\n",
    "    trace = m00 + m11 + m22  \n",
    "\n",
    "    # a) Clamp arguments to sqrt to be ≥ 0\n",
    "    t0 = torch.clamp(1.0 + trace, min=0.0)                   \n",
    "    t1 = torch.clamp(1.0 + m00 - m11 - m22, min=0.0)\n",
    "    t2 = torch.clamp(1.0 - m00 + m11 - m22, min=0.0)\n",
    "    t3 = torch.clamp(1.0 - m00 - m11 + m22, min=0.0)\n",
    "\n",
    "    # b) Compute square roots\n",
    "    sqrt0 = torch.sqrt(t0)  \n",
    "    sqrt1 = torch.sqrt(t1)\n",
    "    sqrt2 = torch.sqrt(t2)\n",
    "    sqrt3 = torch.sqrt(t3)\n",
    "\n",
    "    # c) Assemble each component\n",
    "    qw = 0.5 * sqrt0\n",
    "    qx = 0.5 * torch.sign(m21 - m12) * sqrt1\n",
    "    qy = 0.5 * torch.sign(m02 - m20) * sqrt2\n",
    "    qz = 0.5 * torch.sign(m10 - m01) * sqrt3\n",
    "\n",
    "    quat = torch.stack([qx, qy, qz, qw], dim=-1) \n",
    "\n",
    "    return quat / quat.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "fed68434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.55687259 -0.04613942  0.82931542]\n",
      " [ 0.79003913  0.27876994  0.54600869]\n",
      " [-0.25638074  0.95924891 -0.1187874 ]]\n",
      "tensor([[[-0.5569, -0.0461,  0.8293],\n",
      "         [ 0.7900,  0.2788,  0.5460],\n",
      "         [-0.2564,  0.9592, -0.1188]]])\n",
      "[0.26605679 0.69900464 0.53835751 0.38830077]\n",
      "tensor([[0.2661, 0.6990, 0.5384, 0.3883]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "test_ori = torch.rand_like(torch.zeros((4)))\n",
    "\n",
    "print(R.from_quat(test_ori).as_matrix())\n",
    "print(quaternion_to_matrix(test_ori.unsqueeze(0)))\n",
    "\n",
    "test_matrix = torch.tensor(R.from_quat(test_ori).as_matrix())\n",
    "\n",
    "print(R.from_matrix(test_matrix).as_quat())\n",
    "print(matrix_to_quaternion(test_matrix.unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f92143aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(torch.eye(3), quaternion_to_matrix(test_ori.unsqueeze(0))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "3bc66a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2661, 0.6990, 0.5384, 0.3883]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_to_quaternion(torch.matmul(torch.eye(3), quaternion_to_matrix(test_ori.unsqueeze(0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "7a7e8833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2661, 0.6990, 0.5384, 0.3883])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_to_quaternion(torch.matmul(torch.eye(3), quaternion_to_matrix(test_ori)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "2d06bd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "class Sensor2Global(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def sensor2global(self, ori, acc):\n",
    "\n",
    "        global_inertial_frame = torch.eye(3) # hardcoded\n",
    "        og_mat = quaternion_to_matrix(ori)\n",
    "\n",
    "        # global_mat = torch.matmul(global_inertial_frame.T, og_mat)\n",
    "        global_mat = torch.matmul(global_inertial_frame, og_mat)\n",
    "        global_ori = matrix_to_quaternion(global_mat).squeeze(0)       \n",
    "        acc_ref   = torch.matmul(og_mat, acc.unsqueeze(-1)).squeeze(-1)\n",
    "        # global_acc = torch.matmul(global_inertial_frame.T, acc_ref.unsqueeze(-1)).squeeze(-1)\n",
    "        global_acc = torch.matmul(global_inertial_frame, acc_ref.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        return global_ori, global_acc.squeeze(0)\n",
    "    \n",
    "    def forward(self, all_ori, all_acc):\n",
    "\n",
    "        dev1_ori, dev1_acc = self.sensor2global(all_ori[0].unsqueeze(0), all_acc[0])\n",
    "        dev2_ori, dev2_acc = self.sensor2global(all_ori[1].unsqueeze(0), all_acc[1])\n",
    "        dev3_ori, dev3_acc = self.sensor2global(all_ori[2].unsqueeze(0), all_acc[2])\n",
    "        dev4_ori, dev4_acc = self.sensor2global(all_ori[3].unsqueeze(0), all_acc[3])\n",
    "        dev5_ori, dev5_acc = self.sensor2global(all_ori[4].unsqueeze(0), all_acc[4])\n",
    "\n",
    "        all_ori_global = torch.stack([dev1_ori, dev2_ori, dev3_ori, dev4_ori, dev5_ori], dim=0).unsqueeze(0)\n",
    "        all_acc_global = torch.stack([dev1_acc, dev2_acc, dev3_acc, dev4_acc, dev5_acc], dim=0).unsqueeze(0)\n",
    "\n",
    "        return all_ori_global, all_acc_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ad805a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor2global_func = Sensor2Global()\n",
    "sensor2global_func.eval()\n",
    "ori_input = torch.rand((5,4))\n",
    "acc_input = torch.rand((5,3))\n",
    "traced_func = torch.jit.trace(sensor2global_func, example_inputs=(ori_input, acc_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "fa9531f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 1274/1276 [00:00<00:00, 5086.17 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 63.50 passes/s]\n",
      "Running MIL default pipeline:   9%|▉         | 8/89 [00:00<00:01, 71.74 passes/s]/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '1287', of the source model, has been renamed to 'var_1287' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '1292', of the source model, has been renamed to 'var_1292' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL default pipeline: 100%|██████████| 89/89 [00:02<00:00, 36.36 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 53.10 passes/s]\n"
     ]
    }
   ],
   "source": [
    "sensor2global_func_mlpackage = ct.convert(\n",
    "    traced_func,\n",
    "    inputs=[ct.TensorType(shape=ori_input.shape), \n",
    "            ct.TensorType(shape=acc_input.shape)],\n",
    "    convert_to=\"mlprogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "5feffb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor2global_func_mlpackage.save(\"Sensor2Global.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "9a032cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'var_1287': array([[[0.05899048, 0.5283203 , 0.2854004 , 0.7973633 ],\n",
       "         [0.6791992 , 0.1619873 , 0.15344238, 0.69921875],\n",
       "         [0.7011719 , 0.53759766, 0.3256836 , 0.33666992],\n",
       "         [0.2175293 , 0.5493164 , 0.52685547, 0.61083984],\n",
       "         [0.31958008, 0.6220703 , 0.7133789 , 0.043396  ]]], dtype=float32),\n",
       " 'var_1292': array([[[ 0.66064453,  1.3183594 ,  0.05773926],\n",
       "         [ 0.60058594, -0.28735352,  0.15100098],\n",
       "         [ 0.8496094 , -0.01779175, -0.28198242],\n",
       "         [ 0.36816406,  0.56640625,  0.8261719 ],\n",
       "         [-0.2944336 ,  0.4206543 ,  0.24401855]]], dtype=float32)}"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor2global_func_mlpackage.predict(data={\"all_ori\": ori_input, \"all_acc\": acc_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Calibrator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, imu1_ori, oriMean, accMean):\n",
    "        smpl2imu = quaternion_to_rotation_matrix(imu1_ori).view(3, 3).t()\n",
    "\n",
    "        oris = quaternion_to_rotation_matrix(oriMean)\n",
    "        device2bone = smpl2imu.matmul(oris).transpose(1, 2).matmul(torch.eye(3))\n",
    "        acc_offsets = smpl2imu.matmul(accMean.unsqueeze(-1)) \n",
    "\n",
    "        return smpl2imu, device2bone, acc_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6dd117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imu1_ori_input = torch.rand((4,))\n",
    "oriMean_input = torch.rand((5, 4))\n",
    "accMean_input = torch.rand((5, 3))\n",
    "\n",
    "calibrator  = Calibrator()\n",
    "out = calibrator(imu1_ori_input, oriMean_input, accMean_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc675095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not in eval mode. Consider calling '.eval()' on your model prior to conversion\n",
      "Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting PyTorch Frontend ==> MIL Ops:  99%|█████████▉| 298/300 [00:00<00:00, 5315.16 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 379.77 passes/s]\n",
      "Running MIL default pipeline:   0%|          | 0/89 [00:00<?, ? passes/s]/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '299', of the source model, has been renamed to 'var_299' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '302', of the source model, has been renamed to 'var_302' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL default pipeline: 100%|██████████| 89/89 [00:00<00:00, 218.49 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 186.29 passes/s]\n"
     ]
    }
   ],
   "source": [
    "traced_func = torch.jit.trace(calibrator, example_inputs=(imu1_ori_input, oriMean_input, accMean_input))\n",
    "\n",
    "calibrator_package = ct.convert(\n",
    "    traced_func,\n",
    "    inputs=[ct.TensorType(shape=imu1_ori_input.shape), \n",
    "            ct.TensorType(shape=oriMean_input.shape),\n",
    "            ct.TensorType(shape=accMean_input.shape)],\n",
    "    convert_to=\"mlprogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c12c19d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrator_package.save(\"Calibrator.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2772cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobileposer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
