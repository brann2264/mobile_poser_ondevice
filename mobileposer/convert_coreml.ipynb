{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3454c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import socket\n",
    "import threading\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from argparse import ArgumentParser\n",
    "os.environ['PYGAME_HIDE_SUPPORT_PROMPT'] = \"hide\"\n",
    "from pygame.time import Clock\n",
    "import pickle\n",
    "\n",
    "from articulate.math import *\n",
    "from mobileposer.models import *\n",
    "from mobileposer.utils.model_utils import *\n",
    "from mobileposer.config import *\n",
    "\n",
    "import coremltools as ct\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53cd0b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mobileposer.models.rnn import RNN\n",
    "class JointsBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs: N IMUs.\n",
    "    Outputs: 24 Joint positions. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_imu, seq_length):\n",
    "        super().__init__()\n",
    "        # self.joints = net.joints.joints\n",
    "        self.joints = RNN(n_imu, 24 * 3, 256, seq_length)\n",
    "\n",
    "    def forward(self, batch, input_lengths: Tensor):\n",
    "        # forward joint model\n",
    "        joints, _, _ = self.joints(batch)\n",
    "        return joints\n",
    "\n",
    "class PoserBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs: N IMUs.\n",
    "    Outputs: SMPL Pose Parameters (as 6D Rotations).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_output_joints, n_imu, n_reduced, seq_length):\n",
    "        super().__init__()\n",
    "        # self.pose = net.pose.pose\n",
    "        self.pose = RNN(n_output_joints*3 + n_imu, n_reduced*6, 256, seq_length)\n",
    "\n",
    "    def forward(self, batch, input_lengths: Tensor):\n",
    "        # forward the pose prediction model\n",
    "        pred_pose, _, _ = self.pose(batch)\n",
    "        return pred_pose\n",
    "\n",
    "class VelocityBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs: N IMUs.\n",
    "    Outputs: Per-Frame Root Velocity. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_output_joints, n_imu, seq_length):\n",
    "        super().__init__()\n",
    "\n",
    "        # model definitions\n",
    "        # self.vel = net.velocity.vel\n",
    "        self.vel = RNN(n_output_joints * 3 + n_imu, 24 * 3, 256, bidirectional=False, seq_length=seq_length)\n",
    "\n",
    "    def forward(self, batch, h, c, input_lengths:Tensor):\n",
    "        # forward velocity model\n",
    "        vel, _, state = self.vel(batch, (h,c))\n",
    "        h_out, c_out = state[0].detach(), state[1].detach()\n",
    "        return vel, h_out, c_out\n",
    "    \n",
    "class FootContactBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs: N IMUs.\n",
    "    Outputs: Foot Contact Probability ([s_lfoot, s_rfoot]).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_output_joints, n_imu, seq_length):\n",
    "        super().__init__()\n",
    "        # self.footcontact = net.foot_contact.footcontact\n",
    "        self.footcontact = RNN(n_output_joints * 3 + n_imu, 2, 64, seq_length=seq_length)\n",
    "\n",
    "    def forward(self, batch, input_lengths: Tensor):\n",
    "        # forward foot contact model\n",
    "        foot_contact, _, _ = self.footcontact(batch)\n",
    "        return foot_contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7c694c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobilePoserBase(nn.Module):\n",
    "    # def __init__(self, net, body_model, joints_model, pose_model, contact_model, velocity_model, n_reduced, ignored):\n",
    "    #     super().__init__()\n",
    "\n",
    "    #     #constants\n",
    "    #     self.n_reduced = n_reduced\n",
    "    #     self.ignored = ignored\n",
    "\n",
    "    #     #core model layers\n",
    "    #     self.bodymodel = body_model\n",
    "    #     self.joints = joints_model\n",
    "    #     self.pose = pose_model\n",
    "    #     self.foot_contact = contact_model\n",
    "    #     self.velocity = velocity_model\n",
    "    def __init__(self, n_reduced, ignored, n_imu, n_output_joints, seq_length):\n",
    "        super().__init__()\n",
    "\n",
    "        #constants\n",
    "        self.n_reduced = n_reduced\n",
    "        self.ignored = ignored\n",
    "\n",
    "        #core model layers\n",
    "        self.joints = JointsBase(n_imu=n_imu, seq_length=seq_length)\n",
    "        self.pose = PoserBase(n_imu=n_imu, n_output_joints=n_output_joints, n_reduced=n_reduced, seq_length=seq_length)\n",
    "        self.foot_contact = FootContactBase(n_output_joints=n_output_joints, n_imu=n_imu, seq_length=seq_length)\n",
    "        self.velocity = VelocityBase(n_imu=n_imu, n_output_joints=n_output_joints, seq_length=seq_length)\n",
    "    \n",
    "    def global_to_local_pose(self, pose: torch.Tensor) -> torch.Tensor:\n",
    "        # this runs in Python only\n",
    "        return self.bodymodel.inverse_kinematics_R(pose)\n",
    "\n",
    "    def forward(self, batch, h, c, input_lengths):\n",
    "        # forward the joint prediction model\n",
    "        pred_joints = self.joints(batch, input_lengths)\n",
    "\n",
    "        # forward the pose prediction model\n",
    "        pose_input = torch.cat((pred_joints, batch), dim=-1)\n",
    "        pred_pose = self.pose(pose_input, input_lengths)\n",
    "\n",
    "        # forward the foot-ground contact probability model\n",
    "        tran_input = torch.cat((pred_joints, batch), dim=-1)\n",
    "        foot_contact = self.foot_contact(tran_input, input_lengths)\n",
    "\n",
    "        # foward the foot-joint velocity model\n",
    "        pred_vel, velocity_h, velocity_c = self.velocity(tran_input, h, c, input_lengths)\n",
    "        pred_vel = pred_vel.squeeze(0)\n",
    "\n",
    "        pred_pose, pred_joints, pred_vel, foot_contact = self.process_base_outputs(pred_pose, pred_joints, pred_vel, foot_contact)\n",
    "\n",
    "        return pred_pose, pred_joints, pred_vel, foot_contact, velocity_h, velocity_c\n",
    "    \n",
    "    def process_base_outputs(self, pose, pred_joints, vel, contact):\n",
    "        \n",
    "        pose = art.math.r6d_to_rotation_matrix(pose).view(-1, 24, 3, 3)\n",
    "\n",
    "        # get pose\n",
    "        pose = pose[40].view(-1, 9) #hardcoded num_past_frames = \n",
    "\n",
    "        # compute the joint positions from predicted pose\n",
    "        joints = pred_joints.squeeze(0)[40].view(24, 3)\n",
    "\n",
    "        # compute translation from foot-contact probability\n",
    "        contact = contact[0][40]\n",
    "\n",
    "        # velocity from network-based estimation\n",
    "        root_vel = vel.view(-1, 24, 3)[:, 0]\n",
    "        pred_vel = root_vel[40] / (30/2) #hardcoded fps = 30, vel_scale = 2\n",
    "\n",
    "        # Need to implement in Swift\n",
    "\n",
    "        # lfoot_pos, rfoot_pos = joints[10], joints[11]\n",
    "        # if contact[0] > contact[1]:\n",
    "        #     contact_vel = self.last_lfoot_pos - lfoot_pos + self.gravity_velocity\n",
    "        # else:\n",
    "        #     contact_vel = self.last_rfoot_pos - rfoot_pos + self.gravity_velocity\n",
    "        # weight = self._prob_to_weight(contact.max())\n",
    "        # velocity = art.math.lerp(pred_vel, contact_vel, weight)\n",
    "        # current_foot_y = self.current_root_y + min(lfoot_pos[1].item(), rfoot_pos[1].item())\n",
    "        # if current_foot_y + velocity[1].item() <= self.floor_y:\n",
    "        #     velocity[1] = self.floor_y - current_foot_y\n",
    "\n",
    "        # self.current_root_y += velocity[1].item()\n",
    "        # self.last_lfoot_pos, self.last_rfoot_pos = lfoot_pos, rfoot_pos\n",
    "        # self.last_root_pos += velocity\n",
    "\n",
    "        return pose, joints, pred_vel, contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "166630d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = MobilePoserBase(\n",
    "                    # n_reduced=joint_set.n_full,\n",
    "                    n_reduced=joint_set.n_reduced,\n",
    "                    ignored=joint_set.ignored,\n",
    "                    n_imu=model_config.n_imu,\n",
    "                    n_output_joints=model_config.n_output_joints,\n",
    "                    seq_length=torch.tensor([model_config.past_frames+model_config.future_frames]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d8653c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobilePoserBase(\n",
       "  (joints): JointsBase(\n",
       "    (joints): RNN(\n",
       "      (rnn): LSTM(256, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "      (linear1): Linear(in_features=60, out_features=256, bias=True)\n",
       "      (linear2): Linear(in_features=512, out_features=72, bias=True)\n",
       "      (dropout): Dropout(p=0.4, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (pose): PoserBase(\n",
       "    (pose): RNN(\n",
       "      (rnn): LSTM(256, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "      (linear1): Linear(in_features=132, out_features=256, bias=True)\n",
       "      (linear2): Linear(in_features=512, out_features=96, bias=True)\n",
       "      (dropout): Dropout(p=0.4, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (foot_contact): FootContactBase(\n",
       "    (footcontact): RNN(\n",
       "      (rnn): LSTM(64, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       "      (linear1): Linear(in_features=132, out_features=64, bias=True)\n",
       "      (linear2): Linear(in_features=128, out_features=2, bias=True)\n",
       "      (dropout): Dropout(p=0.4, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (velocity): VelocityBase(\n",
       "    (vel): RNN(\n",
       "      (rnn): LSTM(256, 256, num_layers=2, batch_first=True)\n",
       "      (linear1): Linear(in_features=132, out_features=256, bias=True)\n",
       "      (linear2): Linear(in_features=256, out_features=72, bias=True)\n",
       "      (dropout): Dropout(p=0.4, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base.load_state_dict(torch.load(\"/Users/brianchen/Research/MobilePoser/mobileposer/checkpoints/model_finetuned.pth\", map_location=\"cpu\"))\n",
    "base.load_state_dict(torch.load(\"/Users/brianchen/Research/MobilePoser/mobileposer/checkpoints/weights.pth\"))\n",
    "base.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feb48c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=MobilePoserBase\n",
       "  (joints): RecursiveScriptModule(\n",
       "    original_name=JointsBase\n",
       "    (joints): RecursiveScriptModule(\n",
       "      original_name=RNN\n",
       "      (rnn): RecursiveScriptModule(original_name=LSTM)\n",
       "      (linear1): RecursiveScriptModule(original_name=Linear)\n",
       "      (linear2): RecursiveScriptModule(original_name=Linear)\n",
       "      (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "    )\n",
       "  )\n",
       "  (pose): RecursiveScriptModule(\n",
       "    original_name=PoserBase\n",
       "    (pose): RecursiveScriptModule(\n",
       "      original_name=RNN\n",
       "      (rnn): RecursiveScriptModule(original_name=LSTM)\n",
       "      (linear1): RecursiveScriptModule(original_name=Linear)\n",
       "      (linear2): RecursiveScriptModule(original_name=Linear)\n",
       "      (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "    )\n",
       "  )\n",
       "  (foot_contact): RecursiveScriptModule(\n",
       "    original_name=FootContactBase\n",
       "    (footcontact): RecursiveScriptModule(\n",
       "      original_name=RNN\n",
       "      (rnn): RecursiveScriptModule(original_name=LSTM)\n",
       "      (linear1): RecursiveScriptModule(original_name=Linear)\n",
       "      (linear2): RecursiveScriptModule(original_name=Linear)\n",
       "      (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "    )\n",
       "  )\n",
       "  (velocity): RecursiveScriptModule(\n",
       "    original_name=VelocityBase\n",
       "    (vel): RecursiveScriptModule(\n",
       "      original_name=RNN\n",
       "      (rnn): RecursiveScriptModule(original_name=LSTM)\n",
       "      (linear1): RecursiveScriptModule(original_name=Linear)\n",
       "      (linear2): RecursiveScriptModule(original_name=Linear)\n",
       "      (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripted_core = torch.jit.script(base)\n",
    "scripted_core.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4399d959",
   "metadata": {},
   "outputs": [],
   "source": [
    "bodymodel = art.model.ParametricModel(paths.smpl_file)\n",
    "bodymodel.get_zero_pose_joint_and_vertex()\n",
    "j, _ = bodymodel.get_zero_pose_joint_and_vertex()\n",
    "feet_pos = j[10:12].clone()\n",
    "floor_y = j[10:12, 1].min().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97071827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1283, -0.9559,  0.0750],\n",
       "        [-0.1194, -0.9564,  0.0774]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feet_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45a59fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_lfoot_pos, last_rfoot_pos = (pos for pos in feet_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3db65480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9563523530960083"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "floor_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a440acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_past_frames = model_config.past_frames\n",
    "num_future_frames = model_config.future_frames\n",
    "num_total_frames = num_past_frames + num_future_frames\n",
    "data = torch.zeros((60))\n",
    "imu = data.repeat(num_total_frames, 1)\n",
    "\n",
    "input_length = torch.tensor([num_total_frames])\n",
    "imu_input = imu.unsqueeze(0)\n",
    "h, c = (torch.zeros((2, 1, 256)), torch.zeros((2, 1, 256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d5d36ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 40 is out of bounds for dimension 0 with size 30",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m base\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 3\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mbase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimu_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_length\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mobileposer/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mobileposer/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 48\u001b[0m, in \u001b[0;36mMobilePoserBase.forward\u001b[0;34m(self, batch, h, c, input_lengths)\u001b[0m\n\u001b[1;32m     45\u001b[0m pred_vel, velocity_h, velocity_c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvelocity(tran_input, h, c, input_lengths)\n\u001b[1;32m     46\u001b[0m pred_vel \u001b[38;5;241m=\u001b[39m pred_vel\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m pred_pose, pred_joints, pred_vel, foot_contact \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_base_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_pose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_joints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_vel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfoot_contact\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pred_pose, pred_joints, pred_vel, foot_contact, velocity_h, velocity_c\n",
      "Cell \u001b[0;32mIn[3], line 57\u001b[0m, in \u001b[0;36mMobilePoserBase.process_base_outputs\u001b[0;34m(self, pose, pred_joints, vel, contact)\u001b[0m\n\u001b[1;32m     54\u001b[0m pose \u001b[38;5;241m=\u001b[39m art\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mr6d_to_rotation_matrix(pose)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# get pose\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m pose \u001b[38;5;241m=\u001b[39m \u001b[43mpose\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m9\u001b[39m) \u001b[38;5;66;03m#hardcoded num_past_frames = \u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# compute the joint positions from predicted pose\u001b[39;00m\n\u001b[1;32m     60\u001b[0m joints \u001b[38;5;241m=\u001b[39m pred_joints\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m40\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 40 is out of bounds for dimension 0 with size 30"
     ]
    }
   ],
   "source": [
    "base.eval()\n",
    "with torch.no_grad():\n",
    "    out = base(imu_input, h, c, input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5507d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "When both 'convert_to' and 'minimum_deployment_target' not specified, 'convert_to' is set to \"mlprogram\" and 'minimum_deployment_target' is set to ct.target.iOS15 (which is same as ct.target.macOS12). Note: the model will not run on systems older than iOS15/macOS12/watchOS8/tvOS15. In order to make your model run on older system, please set the 'minimum_deployment_target' to iOS14/iOS13. Details please see the link: https://apple.github.io/coremltools/docs-guides/source/target-conversion-formats.html\n",
      "Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting PyTorch Frontend ==> MIL Ops:  99%|█████████▉| 212/214 [00:00<00:00, 1620.72 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 106.53 passes/s]\n",
      "Running MIL default pipeline:  12%|█▏        | 11/89 [00:00<00:02, 37.15 passes/s]/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '187', of the source model, has been renamed to 'var_187' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '188', of the source model, has been renamed to 'var_188' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '290', of the source model, has been renamed to 'var_290' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '299', of the source model, has been renamed to 'var_299' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '305', of the source model, has been renamed to 'var_305' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '323', of the source model, has been renamed to 'var_323' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL default pipeline: 100%|██████████| 89/89 [00:01<00:00, 80.40 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 268.82 passes/s]\n"
     ]
    }
   ],
   "source": [
    "scripted_core = scripted_core.eval()\n",
    "traced_core = torch.jit.trace(base, (imu_input, h, c, input_length))\n",
    "model_from_trace = ct.convert(\n",
    "    traced_core,\n",
    "    inputs=[ct.TensorType(shape=imu_input.shape), ct.TensorType(shape=h.shape), ct.TensorType(shape=c.shape), ct.TensorType(shape=input_length.shape)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1692e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model as a Core ML model package\n",
    "output_descriptions = [(\"pred_pose\", \"global pose output, need to convert global reduced to local full pose\"),\n",
    "                                        (\"pred_joints\", \"joint predictions\"),\n",
    "                                         (\"pred_vel\", \"velocity predictions\"),\n",
    "                                          (\"foot_contact\", \"foot contact prediction\"),\n",
    "                                           (\"velocity_h\", \"hidden states for velocity lstm\"),\n",
    "                                           (\"velocity_c\", \"initial cell states for velocity lstm\")]\n",
    "model_from_trace.save(\"MobilePoser.mlpackage\")\n",
    "# Load the saved model\n",
    "loaded_model = ct.models.MLModel(\"MobilePoser.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8d3672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = model_from_trace.get_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da9f05a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for out, (name, desc) in zip(model_from_trace._spec.description.output, output_descriptions):\n",
    "    out.shortDescription = desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7bbc549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.transforms import quaternion_to_matrix, matrix_to_quaternion\n",
    "\n",
    "class Quaternion2Matrix(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def quaternion_to_matrix(self, quaternions: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        r, i, j, k = torch.unbind(quaternions, -1)\n",
    "        # pyre-fixme[58]: `/` is not supported for operand types `float` and `Tensor`.\n",
    "        two_s = 2.0 / (quaternions * quaternions).sum(-1)\n",
    "\n",
    "        o = torch.stack(\n",
    "            (\n",
    "                1 - two_s * (j * j + k * k),\n",
    "                two_s * (i * j - k * r),\n",
    "                two_s * (i * k + j * r),\n",
    "                two_s * (i * j + k * r),\n",
    "                1 - two_s * (i * i + k * k),\n",
    "                two_s * (j * k - i * r),\n",
    "                two_s * (i * k - j * r),\n",
    "                two_s * (j * k + i * r),\n",
    "                1 - two_s * (i * i + j * j),\n",
    "            ),\n",
    "            -1,\n",
    "        )\n",
    "        return o.reshape(quaternions.shape[:-1] + (3, 3))\n",
    "\n",
    "    def forward(self, quat):\n",
    "        return self.quaternion_to_matrix(quat)\n",
    "        # # ori, calibration_quats: (..., 4)  quats in (x, y, z, w) order\n",
    "        # # acc:                   (..., 3)\n",
    "        # # device_id:             (1,)  – choose which calibration quat to use\n",
    "\n",
    "        # device_mean_quat = calibration_quats     # stays a tensor\n",
    "\n",
    "        # og_mat   = self.quaternion_to_matrix(ori)                   # (..., 3, 3)\n",
    "        # global_if = self.quaternion_to_matrix(device_mean_quat)     # (3, 3)\n",
    "        # return og_mat, global_if\n",
    "\n",
    "        # global_mat = torch.matmul(global_if.T, og_mat)         # R_g←s\n",
    "        # global_ori = matrix_to_quaternion(global_mat)          # (..., 4)\n",
    "\n",
    "        # acc_ref   = torch.matmul(og_mat, acc.unsqueeze(-1)).squeeze(-1)\n",
    "        # global_acc = torch.matmul(global_if.T, acc_ref.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # return global_ori, global_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27cab13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quat2matrix_func = Quaternion2Matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b11f1aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_input = torch.ones((4))\n",
    "acc_input = torch.ones((3))\n",
    "calibration_quat = torch.ones((4))\n",
    "traced_func = torch.jit.trace(quat2matrix_func, example_inputs=(ori_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e52ca95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not in eval mode. Consider calling '.eval()' on your model prior to conversion\n",
      "Converting PyTorch Frontend ==> MIL Ops:  99%|█████████▊| 72/73 [00:00<00:00, 3658.53 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 1108.67 passes/s]\n",
      "Running MIL default pipeline:   0%|          | 0/89 [00:00<?, ? passes/s]/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '77', of the source model, has been renamed to 'var_77' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL default pipeline: 100%|██████████| 89/89 [00:00<00:00, 665.85 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 1553.40 passes/s]\n"
     ]
    }
   ],
   "source": [
    "quat2matrix_func_from_trace = ct.convert(\n",
    "    traced_func,\n",
    "    inputs=[ct.TensorType(shape=ori_input.shape)],\n",
    "    convert_to=\"mlprogram\")\n",
    "quat2matrix_func_from_trace.save(\"Quat2Matrix.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92f87b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.transforms import quaternion_to_matrix, matrix_to_quaternion, standardize_quaternion\n",
    "\n",
    "class Sensor2Global(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def quaternion_to_matrix(self, quaternions: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        r, i, j, k = torch.unbind(quaternions, -1)\n",
    "        # pyre-fixme[58]: `/` is not supported for operand types `float` and `Tensor`.\n",
    "        two_s = 2.0 / (quaternions * quaternions).sum(-1)\n",
    "\n",
    "        o = torch.stack(\n",
    "            (\n",
    "                1 - two_s * (j * j + k * k),\n",
    "                two_s * (i * j - k * r),\n",
    "                two_s * (i * k + j * r),\n",
    "                two_s * (i * j + k * r),\n",
    "                1 - two_s * (i * i + k * k),\n",
    "                two_s * (j * k - i * r),\n",
    "                two_s * (i * k - j * r),\n",
    "                two_s * (j * k + i * r),\n",
    "                1 - two_s * (i * i + j * j),\n",
    "            ),\n",
    "            -1,\n",
    "        )\n",
    "        return o.reshape(quaternions.shape[:-1] + (3, 3))\n",
    "    \n",
    "\n",
    "    def forward(self, ori, acc, calibration_quats):\n",
    "        # # ori, calibration_quats: (..., 4)  quats in (x, y, z, w) order\n",
    "        # # acc:                   (..., 3)\n",
    "        # # device_id:             (1,)  – choose which calibration quat to use\n",
    "\n",
    "        device_mean_quat = calibration_quats     # stays a tensor\n",
    "\n",
    "        og_mat   = self.quaternion_to_matrix(ori)                   # (..., 3, 3)\n",
    "        global_if = self.quaternion_to_matrix(device_mean_quat)     # (3, 3)\n",
    "\n",
    "        global_mat = torch.matmul(global_if.T, og_mat)         # R_g←s\n",
    "        global_ori = matrix_to_quaternion_single(global_mat)          # (..., 4)\n",
    "        acc_ref   = torch.matmul(og_mat, acc.unsqueeze(-1)).squeeze(-1)\n",
    "        global_acc = torch.matmul(global_if.T, acc_ref.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        return global_ori, global_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80c01351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sensor2Global()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor2global_func = Sensor2Global()\n",
    "sensor2global_func.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba073dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting PyTorch Frontend ==> MIL Ops:  99%|█████████▉| 336/338 [00:00<00:00, 5293.61 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 277.02 passes/s]\n",
      "Running MIL default pipeline:   0%|          | 0/89 [00:00<?, ? passes/s]/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '335', of the source model, has been renamed to 'var_335' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/brianchen/miniconda3/envs/mobileposer/lib/python3.9/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '346', of the source model, has been renamed to 'var_346' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL default pipeline: 100%|██████████| 89/89 [00:00<00:00, 152.30 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 284.24 passes/s]\n"
     ]
    }
   ],
   "source": [
    "traced_func = torch.jit.trace(sensor2global_func, example_inputs=(ori_input, acc_input, calibration_quat))\n",
    "sensor2global_func_from_trace = ct.convert(\n",
    "    traced_func,\n",
    "    inputs=[ct.TensorType(shape=ori_input.shape), ct.TensorType(shape=acc_input.shape), ct.TensorType(shape=calibration_quat.shape)],\n",
    "    convert_to=\"mlprogram\")\n",
    "quat2matrix_func_from_trace.save(\"Sensor2Global.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97907727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def matrix_to_quaternion_single(matrix: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args\n",
    "    ----\n",
    "    matrix : torch.Tensor, shape (3, 3)\n",
    "             Rotation matrix  (rows are destination axes).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    quat   : torch.Tensor, shape (4,)  --  (w, x, y, z) with ||quat|| == 1\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------\n",
    "    # 1.  Pull out the nine elements\n",
    "    # ------------------------------\n",
    "    m00, m01, m02 = matrix[0, 0], matrix[0, 1], matrix[0, 2]\n",
    "    m10, m11, m12 = matrix[1, 0], matrix[1, 1], matrix[1, 2]\n",
    "    m20, m21, m22 = matrix[2, 0], matrix[2, 1], matrix[2, 2]\n",
    "\n",
    "    one = matrix.new_tensor(1.0)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 2.  Same four “absolute” terms as the original function\n",
    "    # -------------------------------------------------------\n",
    "    q_abs = torch.sqrt(\n",
    "        torch.clamp(\n",
    "            torch.stack(\n",
    "                [\n",
    "                    one + m00 + m11 + m22,        # w‑candidate\n",
    "                    one + m00 - m11 - m22,        # x‑candidate\n",
    "                    one - m00 + m11 - m22,        # y‑candidate\n",
    "                    one - m00 - m11 + m22,        # z‑candidate\n",
    "                ]\n",
    "            ),\n",
    "            min=0.0,\n",
    "        )\n",
    "    )                                           # shape (4,)\n",
    "\n",
    "    quat_by_rijk = torch.stack(\n",
    "        [\n",
    "            torch.stack([q_abs[0] ** 2,\n",
    "                         m21 - m12,\n",
    "                         m02 - m20,\n",
    "                         m10 - m01]),\n",
    "            torch.stack([m21 - m12,\n",
    "                         q_abs[1] ** 2,\n",
    "                         m10 + m01,\n",
    "                         m02 + m20]),\n",
    "            torch.stack([m02 - m20,\n",
    "                         m10 + m01,\n",
    "                         q_abs[2] ** 2,\n",
    "                         m12 + m21]),\n",
    "            torch.stack([m10 - m01,\n",
    "                         m20 + m02,\n",
    "                         m21 + m12,\n",
    "                         q_abs[3] ** 2]),\n",
    "        ],\n",
    "        dim=0,                                   # shape (4, 4)\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 4.  Divide each row by the same “safe” denominator\n",
    "    # ---------------------------------------------------\n",
    "    floor = matrix.new_tensor(0.1)\n",
    "    denom = 2.0 * torch.max(q_abs, floor)        # shape (4,)\n",
    "    quat_candidates = quat_by_rijk / denom.unsqueeze(-1)\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 5.  Pick the best‑conditioned row (largest q_abs element)\n",
    "    #     – use tensor‑indexing so the tracer records a gather\n",
    "    # ----------------------------------------------------------\n",
    "    best_idx = torch.argmax(q_abs)               # tensor scalar\n",
    "    quat = quat_candidates[best_idx]             # shape (4,)\n",
    "\n",
    "    # 6.  Normalize to exactly unit length and return (w, x, y, z)\n",
    "    return quat / quat.norm(p=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4b482478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from typing import List\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RotationMatrixToAxisAngle(nn.Module):\n",
    "    def forward(self, r: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param r: Tensor of shape (..., 3, 3), a batch of rotation matrices\n",
    "        :return: Tensor of shape (..., 3), the corresponding axis-angle vectors\n",
    "        \"\"\"\n",
    "        # Flatten batch dims\n",
    "        R = r.view(-1, 3, 3)\n",
    "\n",
    "        # 1) compute the trace → cos θ\n",
    "        tr = R[..., 0, 0] + R[..., 1, 1] + R[..., 2, 2]\n",
    "        cos_theta = (tr - 1.0) * 0.5\n",
    "        cos_theta = cos_theta.clamp(-1.0, 1.0)\n",
    "\n",
    "        # 2) recover θ\n",
    "        theta = torch.acos(cos_theta)\n",
    "\n",
    "        # 3) compute the \"cross-differences\" v = [R32-R23, R13-R31, R21-R12]\n",
    "        rx = R[..., 2, 1] - R[..., 1, 2]\n",
    "        ry = R[..., 0, 2] - R[..., 2, 0]\n",
    "        rz = R[..., 1, 0] - R[..., 0, 1]\n",
    "        v   = torch.stack((rx, ry, rz), dim=-1)\n",
    "\n",
    "        # 4) normalize to get the rotation axis: axis = v / (2 sin θ)\n",
    "        sin_theta = torch.sin(theta).clamp(min=1e-6).unsqueeze(-1)\n",
    "        axis = v / (2.0 * sin_theta)\n",
    "\n",
    "        # 5) axis-angle vector = axis * θ\n",
    "        rot_vec = axis * theta.unsqueeze(-1)\n",
    "\n",
    "        # un-flatten to (..., 3)\n",
    "        return rot_vec.view(*r.shape[:-2], 3)\n",
    "\n",
    "\n",
    "class RodriguesFunc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, r: torch.Tensor):\n",
    "        r\"\"\"\n",
    "        Turn rotation matrices into axis-angles. (torch, batch)\n",
    "\n",
    "        :param r: Rotation matrix tensor that can reshape to [batch_size, 3, 3].\n",
    "        :return: Axis-angle tensor of shape [batch_size, 3].\n",
    "        \"\"\"\n",
    "        result = [cv2.Rodrigues(_)[0] for _ in r.clone().detach().cpu().view(-1, 3, 3).numpy()]\n",
    "        result = torch.from_numpy(np.stack(result)).float().squeeze(-1)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7726fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = torch.rand_like(torch.zeros((3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "55f5922d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3668, 0.4553, 0.4322])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.8145, 0.8596, 0.7993]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = RotationMatrixToAxisAngle()\n",
    "rotation_func = RodriguesFunc()\n",
    "\n",
    "print(new(test_tensor))\n",
    "rotation_func(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a971a453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nx/23kzl3_d08d89039y9hptpf40000gn/T/ipykernel_34131/566892480.py:40: TracerWarning: Converting a tensor to a NumPy array might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  result = [cv2.Rodrigues(_)[0] for _ in r.clone().detach().cpu().view(-1, 3, 3).numpy()]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tracer cannot infer type of [[0.814464   0.85956454 0.7993293 ]]\n:Only tensors and (possibly nested) tuples of tensors, lists, or dictsare supported as inputs or outputs of traced functions, but instead got value of type ndarray.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m rotation_func \u001b[38;5;241m=\u001b[39m RodriguesFunc()\n\u001b[1;32m      2\u001b[0m rotation_func\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 4\u001b[0m traced_func \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrotation_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m rodrigues_func_from_trace \u001b[38;5;241m=\u001b[39m ct\u001b[38;5;241m.\u001b[39mconvert(\n\u001b[1;32m      6\u001b[0m     traced_func,\n\u001b[1;32m      7\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m[ct\u001b[38;5;241m.\u001b[39mTensorType(shape\u001b[38;5;241m=\u001b[39mtest_tensor\u001b[38;5;241m.\u001b[39mshape)],\n\u001b[1;32m      8\u001b[0m     convert_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlprogram\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mobileposer/lib/python3.9/site-packages/torch/jit/_trace.py:798\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_kwarg_inputs should be a dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 798\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs_is_kwarg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m ):\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m example_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mobileposer/lib/python3.9/site-packages/torch/jit/_trace.py:1065\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1064\u001b[0m     example_inputs \u001b[38;5;241m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m-> 1065\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_method_from_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar_lookup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43margument_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1076\u001b[0m check_trace_method \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_c\u001b[38;5;241m.\u001b[39m_get_method(method_name)\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;66;03m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tracer cannot infer type of [[0.814464   0.85956454 0.7993293 ]]\n:Only tensors and (possibly nested) tuples of tensors, lists, or dictsare supported as inputs or outputs of traced functions, but instead got value of type ndarray."
     ]
    }
   ],
   "source": [
    "\n",
    "rotation_func.eval()\n",
    "\n",
    "traced_func = torch.jit.trace(rotation_func, example_inputs=(test_tensor))\n",
    "rodrigues_func_from_trace = ct.convert(\n",
    "    traced_func,\n",
    "    inputs=[ct.TensorType(shape=test_tensor.shape)],\n",
    "    convert_to=\"mlprogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bbfdd8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'const_1': array([[1.7409908, 1.4804977, 1.3079976]], dtype=float32)}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rodrigues_func_from_trace.predict({\"r\": test_tensor})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobileposer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
